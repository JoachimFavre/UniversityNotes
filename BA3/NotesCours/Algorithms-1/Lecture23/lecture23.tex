% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-12-12 at 17:03:38.

\usepackage{../../style}

\title{Algorithms}
\author{Joachim Favre}
\date{Lundi 12 d√©cembre 2022}

\begin{document}
\maketitle

\lecture{23}{2022-12-12}{Quantum bogosort is a comparison sort in $\Theta\left(n\right)$}{
\begin{itemize}[left=0pt]
    \item Proof of an upper bound on the runtime complexity of successful search in hash tables.
    \item Explanation of quicksort.
    \item Proof and analysis of naive quick sort.
    \item Analysis of randomised quick sort.
    \item Proof of the $\Omega\left(n\log\left(n\right)\right)$ lower bound for comparison sorts.
\end{itemize}
}

\begin{parag}{Successful search}
    A successful search also takes expected time $\Theta\left(1 + \alpha\right)$.

    \begin{subparag}{Proof}
        Let's say we search for a uniformly random element. However, a list with many elements has higher probability of being chosen. This means that we need to be careful.

        Let $x$ be the element we search, selected at random amongst all the $n$ elements from the table. The number of elements examined during a successful search for $x$ is one more than the number of elements that appear before $x$ in the list. By the implementation of our table, these elements are the elements inserted after $x$ was inserted, which have the same hash. Thus, we need to find the average of how many elements were inserted into $x$'s list after $x$ was inserted, over the $n$ possibilities to take $x$ in the table.

        For $i = 1, \ldots, n$, let $x_i$ be the $i$\Th element inserted into the table, and let $k_i = \text{key}\left(x_i\right)$. For all $i$ and $j$, we define the following indication variable: 
        \[X_{ij} = I\left(h\left(k_i\right) = h\left(k_j\right)\right)\]
        
        Since we are using simple uniform hashing, $\prob\left(h\left(k_i\right) = h\left(k_j\right)\right) = \frac{1}{m}$, and thus $\exval\left(X_{ij}\right) = \frac{1}{m}$. This tells us that the expected number of elements examined in a successful search is given by: 
        \autoeq{\exval\left(\frac{1}{n} \sum_{i=1}^{n} \left(1 + \sum_{j=i+1}^{n} X_{ij}\right)\right) = \frac{1}{n} \sum_{i=1}^{n} \left(1 + \sum_{j=i+1}^{n} \exval\left(X_{ij}\right)\right) = \frac{1}{n} \sum_{i=1}^{n} \left(1 + \sum_{j=i+1}^{n} \frac{1}{m}\right) = \ldots = 1 + \frac{\alpha}{2} - \frac{\alpha}{2n}}
        which is indeed $\Theta\left(1 + \alpha\right)$
        
        \qed
    \end{subparag}
\end{parag}

\begin{parag}{Complexity of search}
    Both successful and unsuccessful searches have average complexity $\Theta\left(1 + \alpha\right)$, where $\alpha = \frac{n}{m}$. So, if we choose the size of our table to be proportional to the number of elements stored (meaning $m = \Theta\left(n\right)$), then search has $O\left(1\right)$ average complexity.

    This allowed us to construct a table which has insertion and deletion in $O\left(1\right)$, and search in expected $O\left(1\right)$ time.
\end{parag}

\section{Back to sorting}
\subsection{Quick sort}
\begin{parag}{Idea}
    The idea is to again use divide and conquer, but in a way that allows to sort the array in place. So, at any iteration, we choose a value (which yields an index $q$ such that the following properties can be satisfied) named pivot, and we partition $A\left[p\ldots r\right]$ into two (one of them possibly empty) subarrays $A\left[p\ldots \left(q+1\right)\right]$ and $A\left[\left(q+1\right) \ldots r\right]$, such that the elements in the first subarray are less than or equal to $A\left[q\right]$, and the ones in the right are greater than or equal to $A\left[q\right]$. We can then sort each subarray recusrively, and use the fact that we are working in place for not needing any combine step.

    \imagehere[0.6]{QuickSortExample.png}
\end{parag}


\begin{filecontents*}[overwrite]{NaivePartitionQuickSort.code}
procedure Partition(A, p, r):
    pivot = A[r]
    i = p-1  // will be incremented before usage
    for j = p to r-1:
        if A[j] <= pivot:
            swap(A, i, j)  // exchange A[i] with A[j]
    swap(A, i+1, r)  // place pivot correctly by swapping A[i+1] and A[r]
    return i+1  // pivot index
\end{filecontents*}

\begin{parag}{Naive partitioning}
    Let's consider the last element to be the pivot. We thus want to reorder our array in place such that the pivot lands in index $q$, and that the two other subarrays have the property mentioned above. 

    To do so, we can use two counters: $i$ and $j$. $j$ goes through all elements one by one, and if it finds an element less than the pivot, it will move $i$ one forward and place this element at position $i$. That way, $i$ will always have elements less than or equal to the pivot before it.
    \importcode{NaivePartitionQuickSort.code}{pseudo}
    
    \begin{subparag}{Example}
        For instance, it turns the first array to the second one:
        \imagehere[0.9]{QuickSortPartitionExample.png}
    \end{subparag}

    \begin{subparag}{Proof}
        Let's show that our partition algorithm is correct by showing the loop invariant that all entries in $A\left[p \ldots i\right]$ are less than or equal to the pivot, that all entries in $A\left[\left(i+1\right) \ldots \left(j-1\right)\right]$ are strictly greater than the pivot, and that $A\left[r\right]$ is always equal to the pivot.

        The initial step is trivial since, before the loop starts, $A\left[r\right]$ is the pivot by construction, and $A\left[p \ldots i\right]$ and $A\left[\left(i+1\right)\ldots\left(j-1\right)\right]$ are empty.

        For the inductive step, let us split our proof in different cases, letting $x$ to be the value of the pivot. If $A\left[j\right] \leq x$, then $A\left[j\right]$ and $A\left[i+1\right]$ are swapped, and then $i$ and $j$ are incremented; keeping our properties as required. If $A\left[j\right] > x$, then we only increment $j$, which is correct too. In both cases, we don't touch at the pivot $A\left[r\right]$.

        When the loop terminates, $j=r$, so all elements in $A$ are partitioned into $A\left[p \ldots i\right]$ which only has value less than or equal to the pivot, $A\left[\left(i+1\right) \ldots \left(r-1\right)\right]$ which only has value strictly greater than the pivot, and $A\left[r\right]$ being the pivot, showing our loop invariant.

        We can then move the pivot the right place by swapping $A\left[i+1\right]$ and $A\left[r\right]$.

        \qed
    \end{subparag}

    \begin{subparag}{Complexity}
        Let's consider the time complexity of our procedure. 

        The for loop runs around $n = r - p + 1$ times. Each iteration takes time $\Theta\left(1\right)$, meaning that the total running time is $\Theta\left(n\right)$ for an array of length $n$. We can also observe that the number of comparions made is around $n$.
    \end{subparag}
\end{parag}

\begin{filecontents*}[overwrite]{quicksort.code}
procedure Quicksort(A, p, r):
    if p < r:
        q = Partition(A, p, r)
        Quicksort(A, p, q-1)
        Quicksort(A, q+1, r)
        // no need for a combine step :D
\end{filecontents*}

\begin{parag}{Naive quick sort}
    We can now write our quick sort procedure:
    \importcode{quicksort.code}{pseudo}
    
    \begin{subparag}{Worst case}
        Let's consider the worst case running time of this algorithm. If the list is already sorted, we will always have one of our subarray empty:
        \imagehere[0.5]{WorstCaseQuickSort.png}

        Since the \texttt{Partition} procedure takes time proportional to the list size, we get a worst case complexity of: 
        \[\Theta\left(n\right) + \Theta\left(n-1\right) + \ldots + \Theta\left(1\right) = \Theta\left(n^2\right)\]
        which is really not good.
    \end{subparag}
    
    \begin{subparag}{Best case}
        Let's consider the best case of our algorithm now. This happens when every subarray are completely balance every time, meaning that the pivots always split the array into two subarrays of equal size. This gives us the following recurrence: 
        \[T\left(n\right) = 2T\left(\frac{n}{2}\right) + \Theta\left(n\right)\]
        
        We have already seen it, typically for merge sort, and it can be solved to $T\left(n\right) = \Theta\left(n\log\left(n\right)\right)$ by the master theorem.
    \end{subparag}

    \begin{subparag}{Average case}
        Let's now consider the average case over all possible inputs. Doing it formally would take too much time, but let's consider some intuition to get why we have an average complexity of $\Theta\left(n \log\left(n\right)\right)$.

        First, we notice that even if \texttt{Partition} always produces a 9-to-1 split (9 times more elements in one subarray), then we get the recurrence $T\left(n\right) = T\left(\frac{9n}{10}\right) + T\left(\frac{n}{10}\right) + \Theta\left(1\right)$, which solves to $\Theta\left(n \log\left(n\right)\right)$.

        Also, we can notice that even if the recursion tree will not always be good, it will usually be a mix of good and bad splits. For instance, having a bad split (splitting $n$ into $0$ and $n-1$) followed by a perfect split takes $\Theta\left(n\right)$, and yields almost the same result as only having a good split (which also takes $\Theta\left(n\right)$).
    \end{subparag}
\end{parag}

\begin{filecontents*}[overwrite]{RandomisedQuicksort.code}
procedure RandomisedPartition(A, p, r):
    i = Random(p, r)
    exchange A[r] with A[i]
    return Partition(A, p, r)

procedure RandomisedQuicksort(A, p, r):  // the same
    if p < r:
        q = RandomisedPartition(A, p, r)
        RandomisedQuicksort(A, p, q-1)
        RandomisedQuicksort(A, q+1, r)
\end{filecontents*}

\begin{parag}{Randomised quick sort}
    There is a huge difference between the expected running time over all inputs, and the expected running time for any input. We saw intuitively that the first is $\Theta\left(n\log\left(n\right)\right)$, and by computing a random permutation of our input array, we are able to transform the second into the first. 

    However, let's do a better strategy: instead of computing a permutation, let's pick randomly the pivot in the subarray we are considering.

    \begin{subparag}{Implementation}
        This modification can be changed very easily by just changing the \texttt{Partition} procedure:
        \importcode{RandomisedQuicksort.code}{pseudo}
    \end{subparag}
\end{parag}


\begin{parag}{Proposition: Randomised quick sort expected running time}
    Randomised quick sort has expected running time $O\left(n \log\left(n\right)\right)$ for any input.

    \begin{subparag}{Proof}
        The dominant cost of the algorithm is partitioning. Each call to \texttt{Partition} has cost $\Theta\left(1 + F_i\right)$ where $F_i$ is the number of comparisons in the for loop. Since an element can be a pivot at most once, \texttt{Partition} is called at most $n$ times. 

        This means that, letting $X$ to be the number of comparisons performed in all calls to \texttt{Partition}, the total work done over the entire execution is $O\left(n + X\right)$. We thus want to show that $\exval\left(X\right) = O\left(n\log\left(n\right)\right)$.

        Let's call $z_1, \ldots, z_n$ the elements of $A$, in a way such that $z_1 \leq \ldots \leq z_n$. Also, let $Z_{ij} = \left\{z_i, z_{i+1}, \ldots, z_j\right\}$, $C_{ij}$ be the event that $z_i$ is compared to $z_j$ throughout the execution of the algorithm, and $X_{ij} = I\left(C_{ij}\right)$. 

        As each pair is compared at most once (when one of them is the pivot), the total number of comparisons formed by the algorithm is: 
        \[X = \sum_{i=1}^{n} \sum_{j=i+1}^{n} X_{ij}\]
        
        We want to compute its expected value: 
        \[\exval\left(X\right) = \exval\left(\sum_{i=1}^{n} \sum_{j=i+1}^{n} X_{ij}\right) = \sum_{i=1}^{n} \sum_{j=i+1}^{n} \exval\left(X_{ij}\right) = \sum_{i=1}^{n} \sum_{j=i+1}^{n} \prob\left(C_{ij}\right)\]

        This means that we need to compute the probability that $z_i$ is compared to $z_j$. We notice that if at some point we have a pivot $x$ such that $z_i < x < z_j$, then $z_i$ and $z_j$ will never be compared later (since they will be part of two different subarrays).

        We also notice that if $z_i$ or $z_j$ is chosen before any other element of $Z_{ij}$, then it will be compared to all the elements of $Z_{ij}$, except itself. In other words, the probability that $z_i$ is compared to $z_j$ is the probability that either $z_i$ or $z_j$ is the first element chosen as pivot in $Z_{ij}$. There are $j-i+1$ elements and pivots are chosen randomly and independently. Thus, the probability that any particular one of them is the first one chosen is $\frac{1}{j-i+1}$, and: 
        \[\prob\left(C_{ij}\right) = \frac{2}{j - i + 1}\]
        
        Finally, we can put everything together: 
        \[\exval\left(X\right) = \sum_{i=1}^{n} \sum_{j=i+1}^{n} \prob\left(C_{ij}\right) = \sum_{i=1}^{n} \sum_{j=i+1}^{n} \frac{2}{j - i + 1} = \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k+1}\]

        Which is such that:
        \[\exval\left(X\right) < \sum_{i=1}^{n-1} \sum_{k=1}^{n} \frac{2}{k} = \sum_{i=1}^{n-1} O\left(\log\left(n\right)\right) = O\left(n\log\left(n\right)\right)\]
        as required.

        \qed
    \end{subparag}
\end{parag}

\begin{parag}{Remark}
    We have thus seen that randomised quick sort has an expected running time $O\left(n\log\left(n\right)\right)$. Also, this algorithm is in-place and it is very efficient and easy to implement in practice.
\end{parag}

\subsection{Sorting lower bound}
\begin{parag}{Theorem: Sorting lower bound}
    Any sorting algorithm takes $\Omega\left(n\right)$.

    \begin{subparag}{Proof}
        This is trivial since our algorithm definitely needs to consider every input at least once.

        \qed
    \end{subparag}
    
\end{parag}

\begin{parag}{Definition: Comparison sorting}
    A \important{comparison sorting} algorithm is a sorting algorithm which only uses comparisons of two elements to gain order information about a sequence. 

    \begin{subparag}{Remark}
        All sorts we have seen so far (insertion sort, merge sort, heapsort, and quicksort) are comparison sorts.
    \end{subparag}
\end{parag}


\begin{parag}{Theorem: Comparison sorting lower bound}
    Any comparison sorting takes (expected time) $\Omega\left(n\log\left(n\right)\right)$.

    \begin{subparag}{Proof}
        Any comparison sorting algorithm can be summed up by a decision tree. 
        \imagehere[0.7]{SortingDecisionTree.png}

        We have $n!$ possible permutations of our numbers, meaning at least $n!$ leafs. The goal of a sorting algorithm is to find which permutation is the sorted one, meaning to find the path to the correct leaf. At any step, we can either see that the operation $a < b$ is true or false, meaning that our tree is a binary tree. This leads that the height of our tree is at least: 
        \[\Omega\left(\log_2\left(n!\right)\right) = \Omega\left(n\log\left(n\right)\right)\]
        
        However, by definition of the height of a tree, it means that some of its leafs are at this depth, and thus that there exists input permutation so that we need to do $\Omega\left(n\log\left(n\right)\right)$ comparisons.

        In fact, a similar argument implies that $\Omega\left(n\log\left(n\right)\right)$ comparisons are necessary for most inputs.
        \qed
    \end{subparag}
    

    \begin{subparag}{Remark}
        In some sense, it means that merge-sort, heapsort and quicksort are optimal.
    \end{subparag}
\end{parag}

\begin{filecontents*}[overwrite]{CountingSort.code}
procedure CountingSort(A, B, n, k):
    // Initialise i
    let C[0...k] be a new array
    for i = 0 to k:
        C[i] = 0
    
    // Count occurences
    for j = 1 to n:
        C[A[j]] += 1

    // Turn to cumulative sum
    for i = 1 to k:
        C[i] += C[i-1]

    // Put the values in order in B
    for j = n downto 1:
        B[C[A[j]]] = A[j]  // place i in B[C[i]]
        C[A[j]] -= 1  // if needs to be placed again, must not be at the same place
\end{filecontents*}

\begin{parag}{Counting sort}
    Let's try to make a sorting algorithm in $O\left(n\right)$. By our theorem above, it requires not to be a comparison sort.

    Let's say that the array we receive $A\left[1\ldots n\right]$ has values $A\left[j\right] \in \left\{0, 1, \ldots, k\right\}$ for some $k$. Note that, as long as the array has integer values, we can always shift it like that by subtracting the minimum (found in $O\left(n\right)$). 

    Now, the idea, is to make an array $C\left[0\ldots k\right]$ such that $C\left[i\right]$ represents the number of times the value $i$ appears in $A$ (it can be 0, or more). It can be constructed from $A$ in one pass. From this array, we can then make a new, sorted, array $B\left[1 \ldots n\right]$ with the values of $A$, in another single pass.

    \begin{subparag}{Algorithm}
        We can realise that adding one more step can make the algorithm much simpler to implement. When we have computed our array $C$, we can then turn it to a cumulative sum, so that $C\left[i\right]$ represents the number of times the values $0, \ldots, i$ appear in $A$. However, we notice that, then, this values is exactly the place value $i$ would require to land in $B$ (if we need to add it to $B$), i.e. $B\left[C\left[i\right]\right] = i$.

        \importcode{CountingSort.code}{pseudo}
    \end{subparag}
    
    \begin{subparag}{Analysis}
        Our algorithm only consists of for loops with $\Theta\left(k\right)$ iterations and $\Theta\left(n\right)$ iterations. This gives us a runtime complexity of $\Theta\left(n + k\right)$.

        Note that, in fact, because $k$ typically has to be very big, this is not better than the comparisons sort we saw for arrays we have no information about. However, if this $k$ is small enough, then it can be very powerful.
    \end{subparag}
    
    \begin{subparag}{Remark}
        This algorithm will not be at the exam since we did not have time to see in the lecture.
    \end{subparag}
    
\end{parag}



\end{document}

