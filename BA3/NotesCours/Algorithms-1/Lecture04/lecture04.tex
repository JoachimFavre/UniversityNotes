% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-10-03 at 14:15:42.

\usepackage{../../style}

\title{Algorithms}
\author{Joachim Favre}
\date{Lundi 03 octobre 2022}

\begin{document}
\maketitle

\lecture{4}{2022-10-03}{Master theorem}{
\begin{itemize}[left=0pt]
    \item Explanation of the master theorem.
    \item Explanation of how to count the number of inversions in an array.
\end{itemize}

}

\begin{parag}{Example}
    Let's look at the following recurrence: 
    \[T\left(n\right) = T\left(\frac{n}{4}\right) + T\left(\frac{3}{4}n\right) + 1\]
    
    We want to show it is $\Theta\left(n\right)$.

    \begin{subparag}{Upper bound}
        Let's prove that there exists a $b$ such that $T\left(n\right) \leq bn$. We consider the base case to be correct, by choosing $b$ to be large enough.

        Let's do the inductive step. We get:
        \autoeq{T\left(n\right) = T\left(\frac{1}{4}n\right) + T\left(\frac{3}{4}n\right) + 1 \leq b \frac{1}{4} n + b \frac{3}{4} n + 1 = bn + 1}
        
        But we wanted $bn$, so it proves nothing. We could consider that our guess is wrong, or do another proof.
    \end{subparag}

    \begin{subparag}{Upper bound (better)}
        Let's now instead the harder induction hypothesis, stating that $T\left(n\right) \leq bn - b'$. This gives us:
        \autoeq{T\left(n\right) = T\left(\frac{1}{4}n\right) + T\left(\frac{3}{4}n\right) + 1 \leq b \frac{1}{4} n - b' + b \frac{3}{4} n - b' + 1 = bn - b' + \left(1 - b'\right) \leq bn - b'}
        as long as $b' \geq 1$. 

        Thus, taking $b$ such that the base case works, we have proven that $T\left(n\right) \leq bn - b' \leq bn$, and thus $T\left(n\right) \in O\left(n\right)$. We needed to make our claim stronger for it to work, and this is something that is often needed to do.
    \end{subparag}
\end{parag}

\begin{parag}{Master theorem}
    Let $a \leq 1$ and $b > 1$ be constants. Also, let $T\left(n\right)$ be a function defined on the nonnegative integers by the following recurrence: 
    \[T\left(n\right) = aT\left(\frac{n}{b}\right) + f\left(n\right)\]
    
    Then, $T\left(n\right)$ has the following asymptotic bounds:
    \begin{enumerate}
        \item If $f\left(n\right) = O\left(n^{\log_b\left(a\right) - \epsilon}\right)$ for some constant $\epsilon > 0$, then $T\left(n\right) = \Theta\left(n^{\log_b\left(a\right)}\right)$.
        \item If $f\left(n\right) = \Theta\left(n^{\log_b\left(a\right)}\right)$, then $T\left(n\right) = \Theta\left(n^{\log_b\left(a\right)} \log\left(n\right)\right)$.
        \item If $f\left(n\right) = \Omega\left(n^{\log_b\left(a\right) + \epsilon}\right)$ for some constant $\epsilon > 0$, and if $a f\left(\frac{n}{b}\right) \leq c f\left(n\right)$ for some constant $c < 1$ and all sufficiently large $n$, then $T\left(n\right)= \Theta\left(f\left(n\right)\right)$. Note that the second condition holds for most functions.
    \end{enumerate}

    \begin{subparag}{Example}
        Let us consider the case for merge sort, thus $T\left(n\right) = 2T\left(\frac{n}{2}\right) + cn$. We get $a = b = 2$, so $\log_b\left(a\right) = 1$ and: 
        \[f\left(n\right) = \Theta\left(n^1\right) = \Theta\left(n^{\log_b\left(a\right)}\right)\]
        
        This means that we are in the second case, telling us: 
        \[T\left(n\right) = \Theta\left(n^{\log_b\left(a\right)} \log\left(n\right)\right) = \Theta\left(n \log\left(n\right)\right)\]
    \end{subparag}
    
    \begin{subparag}{Tree}
        To learn this theorem, we only need to get the intuition of why it works, and to be able to reconstruct it. To do so, we can draw a tree. The depth of this tree is $\log_b\left(n\right)$, and there are $a^{\log_b\left(n\right)} = n^{\log_b\left(a\right)}$ leaves. If a node does $f\left(n\right)$ work, then each of its children does $af\left(\frac{n}{b}\right)$ work.

        \begin{enumerate}[left=0pt]
            \item If $f$ grows slowly, a parent does less work than all its children combined. This means that most of the work is done at the leaf. Thus, the only thing that matters is the number of leafs which $f$ has: $n^{\log_b\left(a\right)}$.
            \item If $f$ grows such that every child contributes exactly the same as their parents, then every level does the same work. Since we have $n^{\log_b\left(a\right)}$ leafs which each contribute a constant amount of work, the last level adds up to $c\cdot n^{\log_b\left(a\right)}$ work, and thus every level adds up to this value. We have $\log_b\left(n\right)$ levels, meaning that we have a total work of $c n^{\log_b\left(a\right)} \log_b\left(n\right)$.
            \item If $f$ grows fast, then a parent does more work than all its children combined. This means that all the work is done at the root and, thus, that all that matters is $f\left(n\right)$.
        \end{enumerate}
    \end{subparag}

\end{parag}

\begin{parag}{Application}
    Let's use a modified version of merge sort in order to count the number of inversions in an array $A$ (an inversion is $i < j$ such that $A\left[j\right] < A\left[i\right]$, where $A$ never has twice the same value).

    The idea is that we can just add a return value to merge sort: the number of inversions. In the trivial case $n = 0$, there is no inversion. For the recursive part, we can just add the number of inversions of the two sub-cases and the number of inversions we get from the merge procedure (which is the complicated part). 

    For the merge procedure, since the two subarrays are sorted, we notice that if the element we are considering from the first subarray is greater than the one we are considering of the second subarray, then we need to add $\left(q - i + 1\right)$ to our current count.
    \svghere{CountInversions.svg}

    This solution is $\Theta\left(n \log\left(n\right)\right)$ and thus much better than the trivial $\Theta\left(n^2\right)$ double for-loop solution.

    \begin{subparag}{Remark}
        We can notice that there are at most $\frac{n\left(n-1\right)}{2}$ inversions (in a reverse-sorted array). It seems great that our algorithm achieves to count this value in a smaller complexity. This comes from the fact that, sometimes, we add much more than 1 at the same time in the merge procedure.
    \end{subparag}
\end{parag}


\end{document}
