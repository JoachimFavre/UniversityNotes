% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2022-12-08 at 23:55:56.

\usepackage{texdate}
\usepackage{../style}

\title{Introduction to machine learning --- BA3 \\ Detailed summary}
\author{Joachim Favre\\ Course by Prof. Mathieu Salzmann}
\date{Autumn semestre 2022}

\begin{document}
\maketitle

\cftsetindents{paragraph}{3em}{2em}
\setcounter{tocdepth}{5}

\tableofcontents

\initcurrdate
\def\setdateformat{Y--m--d}
\vspace*{\fill}
\begin{center}
    \textit{Version \printdate}
\end{center}
\vspace*{\fill}
\newpage

\section{Prerequisites}
\parag{Supervised and unsupervised learning}{
    In \important{supervised learning}, we are given data and its groundtruth labels. The goal is, given new data, we want to predict new labels, by doing regression or classification.

    In \important{unsupervised learning}, we are only given data (without any label), and we want to output some information about it, by doing dimensionality reduction or clustering.
}

\parag{Regression and classification}{
    The goal of \important{regression} is to predict a continuous value for a given sample. The goal of \important{classification} is to output a discrete label (typically encoded in one-hot encoding with 0s and 1s or -1s and 1s).

    The main difference is that there is the notion of closeness in regression (when predicting a date, outputting 1970 when it should be 1980 is better than outputting 2100), which is not in classification (when predicting what is on a picture, outputting a car when it should be a cat is not better or worse than outputting an elephant).
}

\parag{Dimensionality reduction}{
    Dimensionality reduction has two main advantages. 

    The first one is that it allows to decrease the dimension of our data, which typically yield a tremendous speed-up while preserving a lot of the precision.

    The second one is that, depending on the model, we can also map data back from the lower dimensional space to the higher one. This can be very interesting since it allows to create new samples. For instance, applying dimensionality reduction on a set of human faces, we could create new points $\bvec{y}$ randomly thanks to the distribution of our data, and map it back to high dimension. That way, we created a new random face. Another use for this is to denoise the data: mapping a sample to a lower dimensional space and back to high dimensions may result to a lot less noise.
}

\parag{Notations}{
    We consider the following notation throughout this course, with some slight exceptions when specified otherwise. $N$ is the number of samples we have, $D$ is the dimensionality (the number of components) of any input, and $C$ is the dimensionality of any output. 

    Without specified otherwise, the input $\bvec{x}_i \in \mathbb{R}^{D+1}$ begins with a constant $1$ to account for a bias, followed by the input data. We let $\bvec{y}_i \in \mathbb{R}^C$ to be the $i$\Th output. $x_i^{\left(k\right)}$ is the $k$\Th component of the $i$\Th input, and similarly for $y_i^{\left(k\right)}$. To sum up, we have:
    \[\bvec{x}_i = \begin{pmatrix} 1 \\ x_i^{\left(1\right)} \\ \vdots \\ x_i^{\left(D\right)} \end{pmatrix} \in \mathbb{R}^{D+1}, \mathspace \bvec{y}_i = \begin{pmatrix} y_i^{\left(1\right)} \\ \vdots \\ y_i^{\left(C\right)} \end{pmatrix} \in \mathbb{R}^{C}\]

    Any value output by our model will be represented by a $\hat{\bvec{y}} \in \mathbb{R}^{C}$, to make a difference with the groundtruth $\bvec{y} \in \mathbb{R}^C$.
    
    We will also need weights, which represent the parameters of our model. $\bvec{w}_{\left(i\right)}$ represents the weight to convert any $\bvec{x}$ to the $i$\Th component of the output $y_i$. We need their size to compute dot products with $\bvec{x}$, i.e. $\bvec{x}_i^T \bvec{w}_{\left(i\right)}$ must make sense. Thus, without specified otherwise, we use $\bvec{w}_{\left(i\right)} \in \mathbb{R}^{\left(D+1\right)}$.

    To simplify the notation and the computations, we will stack our values in matrices. Thus, we let: 
    \[X = \begin{pmatrix} \bvec{x}_1^T \\ \vdots \\ \bvec{x}_N^T \end{pmatrix} = \begin{pmatrix} 1 & \cdots & x_1^{\left(D\right)} \\ \vdots & \ddots & \vdots \\ 1 & \cdots & x_N^{\left(D\right)} \end{pmatrix} \in \mathbb{R}^{N \times \left(D+1\right)}\]
    \[Y = \begin{pmatrix} \bvec{y}_1^T \\ \vdots \\ \bvec{y}_N^T \end{pmatrix} = \begin{pmatrix} y_1^{\left(1\right)} & \cdots & y_1^{\left(C\right)} \\ \vdots & \ddots & \vdots \\ y_N^{\left(1\right)} & \cdots & y_n^{\left(C\right)} \end{pmatrix} \in \mathbb{R}^{N \times C}\]
    \[W = \begin{pmatrix} \bvec{w}_{\left(1\right)} & \hdots & \bvec{w}_{\left(C\right)} \end{pmatrix} \in \mathbb{R}^{\left(D+1\right)\times C}\]
}

\parag{Feature expansion}{
    Increasing the amount of dimensions from $D$ to $F$ of our input data may help our models (using non-linear functions, since they would be of no help). Thus, we may let the following function: 
    \[\phi\left(\bvec{x}\right) = \begin{pmatrix} 1 & x^{\left(1\right)} & \cdots & x^{\left(D\right)} & \left(x^{\left(1\right)}\right)^2 & \cdots & \left(x^{\left(D\right)}\right)^2 & \cdots \end{pmatrix}^T \in \mathbb{R}^{F}\]
    
    We also put it in a matrix so simplify notation: 
    \[\Phi = \begin{pmatrix} \phi\left(\bvec{x}_1\right)^T \\ \vdots \\ \phi\left(\bvec{x}_N\right)^T \end{pmatrix} \in \mathbb{R}^{N \times F} \]
    
    We can replace $\bvec{x}$ by $\phi\left(\bvec{x}\right)$ and $X$ by $\Phi$ in mostly every model, especially the ones which will be kernalised. Note that, in this case, we need $\bvec{w} \in \mathbb{R}^F$, and thus $W = \mathbb{R}^{F \times C}$.

}

\paragsubparag{}{Remark}{
        The 1 we added to the input data to account for the bias is some kind of feature expansion.
}


\paragsubparag{}{Cover's Theorem}{

        Cover's theorem states (more or less) that doing non-linear feature expansion, then it is more likely for our data to be linearly separable.
}


\parag{Kernel}{
    We can notice that defining our $\phi$ functions for feature expansion can be really tedious. However, since most of our methods depend on a dot product of $\phi\left(\bvec{x}_i\right)^T \phi\left(\bvec{x}_j\right)$, which gives some kind of measure of similarity between $\bvec{x}_i$ and $\bvec{x}_j$ (since it is proportional to the cosine of their angle), we can define a similarity function named a \important{kernel}, such that: 
    \[k\left(\bvec{x}_i, \bvec{x}_j\right) = \phi\left(\bvec{x}_i\right)^T \phi\left(\bvec{x}_j\right)\]

    As usual, we put everything in vectors and matrices to simplify our notation. We first have a way to measure the similarity between a sample $\bvec{x}_i$ and all the other samples:
    \[k\left(X, \bvec{x}_i\right) = \begin{pmatrix} k\left(\bvec{x}_1, \bvec{x}_i\right) \\ \vdots \\ k\left(\bvec{x}_n, \bvec{x}_i\right) \end{pmatrix} \in \mathbb{R}^N\]

    And we can then stack all of them in a matrix to define similarity between all pairs of samples: 
    \[K = \begin{pmatrix} k\left(\bvec{x}_1, \bvec{x}_1\right) & k\left(\bvec{x}_1, \bvec{x}_2\right) & \cdots & k\left(\bvec{x}_1, \bvec{x}_N\right) \\ k\left(\bvec{x}_2, \bvec{x}_1\right) & k\left(\bvec{x}_2, \bvec{x}_2\right) & \cdots & k\left(\bvec{x}_2, \bvec{x}_N\right) \\ \vdots & \vdots & \ddots & \vdots \\ k\left(\bvec{x}_N, \bvec{x}_1\right) & k\left(\bvec{x}_N, \bvec{x}_2\right) & \cdots & k\left(\bvec{x}_N, \bvec{x}_N\right)\end{pmatrix} \in \mathbb{R}^{N \times N}\]
    
    Note that, since $k\left(\bvec{x}_i, \bvec{x}_j\right) = k\left(\bvec{x}_j, \bvec{x}_i\right)$ by the commutativity of the dot product, $K$ is symmetric ($K^T = K$).

}

\paragsubparag{}{Remark}{
        The main advantage of a kernel is that we don't need to know what function $\phi$ is linked to it.
}


\paragsubparag{}{Examples}{
    
        We can for instance use the \important{polynomial kernel}: 
        \[k\left(\bvec{x}_i, \bvec{x}_j\right) = \left(\bvec{x}_i^T \bvec{x}_j + c\right)^d\]
        
        $c$ is often set to $1$ and $d$ to 2. For this kernel, the corresponding mapping $\phi$ is known. This is, except for multiplicative constants, all the possible monomials of degree less than or equal to $d$ composed of the components of $\bvec{x}_i$ and $\bvec{x}_j$.

        We can also use the \important{Gaussian kernel} (or radial basis function (RBF)): 
        \[k\left(\bvec{x}_i, \bvec{x}_j\right) = \exp\left(- \frac{\left\|\bvec{x}_i - \bvec{x}_j\right\|^2}{2 \sigma ^2}\right)\]
        
        $\sigma$ is typically chosen relatively to the data.
}



\parag{Representer theorem}{
    The minimizer of a regularized empirical risk function can be represented as a linear combination of expanded features. In other words, we can write: 
    \[\bvec{w}^* = \sum_{i=1}^{N} \alpha_i^* \phi\left(\bvec{x}_i\right) = \Phi^T \bvec{\alpha}^*\]
    where $\bvec{\alpha} \in \mathbb{R}^N$.

}

\paragsubparag{}{Remark}{
        This theorem is really important to kernalise algorithms. When using it, the goal is to get rid of the $\Phi$ since we do not know the mapping $\phi$. Switching our view onto variables $\bvec{\alpha}$ instead of variables $\bvec{w}$ is typically a way to do so.
}


\parag{Loss function}{
    
    The \important{loss function} $\ell \left(\hat{\bvec{y}}_i, \bvec{y}_i\right)$ computes an error value between the prediction and the true value.

    This is a measure of the error for any given prediction.
}

\parag{Empirical risk}{
    Given $N$ training samples $\left\{\left(\hat{\bvec{x}}_i, \hat{\bvec{y}}_i\right)\right\}$, the \important{empirical risk} is defined as: 
    \[R\left(\left\{\hat{\bvec{x}}_i\right\}, \left\{\hat{\bvec{y}}_i\right\}, W\right) = \frac{1}{N} \sum_{i=1}^{N} \ell \left(\hat{\bvec{y}}_i, \bvec{y}_i\right)\]

    This represents the global error on the training samples, this is what we try to minimise: 
    \[W^* = \argmin_{W} R\left(\left\{\hat{\bvec{x}}_i\right\}, \left\{\hat{\bvec{y}}_i\right\}, W\right)\]

}

\paragsubparag{}{Regularised}{
        Sometimes, we want to regularise our objective function, so that we prevent weights to become to large and make a lot of overfitting. We then instead seek to minimise. 
        \[E\left(W\right) = R\left(W\right) + \lambda E_W\left(W\right)\]
        where $\lambda$ is an hyperparameter and $E_W\left(W\right)$ is the regulariser.

        A regulariser that can be used is Tikhonov regularisation, and it will be used in ridge regressionb: 
        \[E_W\left(W\right) = \left\|W\right\|^2_F\]
        where $\left\|W\right\|_F^2$ is the squared Frobenius norm of $W$, meaning the sum of its values squared.
}


\parag{Gradient descent}{
    The goal of \important{gradient descent} is to minimise a function (an empirical risk $R\left(W\right)$ in this context). The idea is to begin with an estimate $W_0$ (typically completely random), and then to update it iteratively, by following the direction of greatest decrease (the opposite of the gradient):
    \[W_k = W_{k-1} - \eta \nabla_{W} R\left(W_{k-1}\right)\]
    where $\eta > 0$ is the learning rate. 

    This algorithm can then be stopped after the change in the function reaches a threshold $\left|R\left(W_{k}\right) - R\left(W_{k-1}\right)\right| < \delta_R$, the change in parameter value is less than a threshold $\left|W_{k} - W_{k-1}\right| < \delta_w$, or if a maximum number of iterations (also known as epochs) is reached (even though this gives no guarantee on a potential convergence).

}

\paragsubparag{}{Remark}{
        This algorithm does not always converge and, when it does, not necessarily to a minimum nor to the global minimum.
}


\paragsubparag{}{Stochastic gradient descent}{

        When we have a lot of input and a non-convex function (meaning that it probably does not have a single minimum, and thus that gradient descent can loose itself in some of them), we can use \important{stochastic gradient descent}. The idea is, instead of using all our samples, we only use a random subset (named \important{mini-batch}) of $B$ samples. The update rule becomes: 
        \[W_k = W_{k-1} - \eta \sum_{b=1}^{B} \nabla \ell _{i\left(b, k\right)}\left(W_{k-1}\right)\]

        This converges faster for the same number of computations, and it may get out of bad local minima.
}


\parag{Evaluation metrics}{
    Once a supervised machine learning model is trained, we want to be able to understand how well it performs on unseen test data (which must absolutely be separated from the train data). 

    We could use the loss function, but we may also use a different one.

    For regression, we typically use \important{Mean Squared Error} (MSE): 
    \[\text{MSE} = \frac{1}{N_t} \sum_{i=1}^{N_t} \left\|\hat{\bvec{y}}_i - \bvec{y}_i\right\|^2\]
    where $N_t$ is the number of test sample.

    For classification, this is typically more complicated. Defining $TP$ to be the number true positive predictions (samples correctly predicted positive), $FN$ to be the number of false negative predictions (samples incorrectly predicted negative), and similarly for $TN$ and $FP$, we can define the \important{accuracy} (the percentage of correctly classified samples), the \important{precision} (the percentage of samples classified positives, which are truly positives) and the \important{recall} (the percentage of positive samples that are correctly classified as positives): 
    \[\text{accuracy} = \frac{TP + TN}{N_t}, \mathspace \text{precision} = \frac{TP}{TP + FP}, \mathspace \text{recall} = \frac{TP}{P}\]
    
    We can then combine the two last to make a \important{F1 score}: 
    \[\text{F1} = 2 \frac{\text{precision} \cdot  \text{recall}}{\text{precision} + \text{recall}}\]
    
    We can typically then use accuracy and the F1 score to see how well our classification model did.

}

\paragsubparag{}{Remark}{
        There are many more metrics for regression and classification. For the former, we could quote RMSE (root mean squared error), MAE (mean absolute error) or the MAPE (mean absolute percentage error). For the latter, making a confusion matrix or computing the AUC (area under the ROC curve) can give good insights too.
}


\parag{Decision boundary}{
    A classifiers lead a decision boundary. This is an object of dimension $D-1$ (a line if our data lies on a plane for instance), which splits the space into two regions: one where samples are considered positive (the predicted value is closer to the value of positive samples), and one where they are considered to be negative (the predicted value is closer to the value of negative samples).

    Since it is the set of points which are equivalently distant to both labels, the boundary is parametrised by the following equation: 
    \[\hat{\bvec{y}}\left(\bvec{x}\right) = \frac{y_{pos} + y_{neg}}{2}\]
    where $y_{pos}$ is the value for positive samples, and $y_{neg}$ is the value for negative samples.1
}

\paragsubparag{}{Remark}{
        A classifier is said to be linear if its decision boundary is an hyperplane (a straight line if the data lies on a plane for instance).
}


\parag{Margin}{
   If $C = 1$, the orthogonal distance between the decision boundary and the nearest sample is called the margin.
   \imagehere[0.5]{margin.png}
}


\parag{Overfitting}{
    When we increase the complexity of the model (by changing the hyperparameters) we get better and better result for both training and test data. However, there is a point at which increasing the complexity keeps decreasing error on training data but increases the error on test data. This is a very general phenomenon, named \important{overfitting}.
}

\parag{Cross-validation}{
    Cross-validation is a way to find good hyperparameters that prevent overfitting. We test different models (in a predefined set), assign to each of them an error value, and pick the one yielding the smallest error.

    The idea of \important{$k$-fold cross-validation} is, to evaluate the error of a model, to first randomly split the dataset into $k$ partitions (where $k$ is predefined). Then, we do $k$ iterations: at iteration $i$ we drop the $i$\Th partition, train the model on the $\left(k-1\right)$ other folds merged, and use the $i$\Th partition to compute the error. At the end of all the iterations, we average all the errors.

    Note that we never test the model on data we used to train it, allowing to avoid overfitting. Also, we use all of our training data to get a complete insight over it (doing only one iteration, would give less information about the model). It is important to notice that the larger the $k$, the less data we waste but the more expensive this method becomes.

}

\paragsubparag{}{Remark}{
        Note that leaving $k = N$ (where $N$ is the number of training samples) is also sometimes named \important{leave-one-out cross-validation}. This is really expensive but wastes (almost) no data.

        Another way to do cross-validation, which is much cheaper, is to split our training data into training and validation data, using \important{validation-set cross-validation}. This is like like doing only one iteration of $k$-fold cross-validation, meaning that it is very cheap but wastes a lot of data.
}


\parag{Data representation}{
    All the models we will see only work for fixed size data. If we want to handle data of varying size (such as text or pictures), a good way is to consider \important{bag of words}: consider the number of times each word from a dictionary appears in the text and put this as a big vector.

    Note that we don't need to consider the whole English dictionary, only picking the set of words appearing in the training data is enough. Also, it is often interesting to make a histogram out of our vector: divide it by the number of words in the sample, so that we instead have a repartition and give less importance to the length of the text.

    Also, we can apply this to images. To do so, we need to extract ``words'', meaning fixed-size picture elements.
}

\parag{Pre-processing}{
    The training data might have problems: it might have noise (because of measurement errors), incorrect values, and so on. To fix those, a good idea is to do pre-processing.

}

\paragsubparag{}{Normalisation}{
        To begin with, a good idea is to scale each individual feature dimension to fall within a specified range (so that we don't give more impact to a dimension ranging from 1000 to 10000 than to another dimension ranging from 0 to 1). This can typically be done by using \important{normalisation}, such as $z$-score normalisation: 
        \[\widetilde{x}_i^{\left(d\right)} = \frac{x_i^{\left(d\right)} - \mu^{\left(d\right)}}{\sigma^{\left(d\right)}}\]
        where $\mu^{\left(d\right)}$ is the mean of the $d$\Th dimension, and $\sigma^{\left(d\right)}$ is its standard deviation.

        Note that there are many other ways to do normalisation, such as min-max normalisation (computing $\widetilde{x}_i = \frac{x_i - x_{min}}{x_{max} - x_{min}}$), max-normalisation (computing $\widetilde{x}_i = \frac{x_i}{x_{max}}$ where $x_{max}$ is the maximum in absolute value) or decimal-scaling (compute $\widetilde{x}_i = \frac{x_i}{10^k}$ where $k$ is the smallest integer $k$ such that $\left|\widetilde{x}_i\right| \leq 1$ for the largest $\widetilde{x}_i$).
}


\paragsubparag{}{Imbalanced data}{
    
        Another important thing to consider is \important{imbalanced data}. There might be 10 times as much data in one class as in another (between-class imbalance), or data inside a class might have a lot of representative at some point in space and much less at other points (within-class imbalance).

        To fix those problems, we can either work on the data, or on the cost function.

        To work on the data, the first method is to decrease the data set by \important{undersampling}: we remove samples randomly, or more intelligently by removing samples considered redondant. The second method is to increase the data set by \important{oversampling}: we replicate exactly some of the samples (which might lead to overfitting), or do interpolation (which might create a lot of noise).

        To work on the cost function, we can give more impact to some samples: 
        \[R\left(\left\{\bvec{x}_i\right\}, \left\{\bvec{y}_i\right\}, W\right) = \frac{1}{N} \sum_{i=1}^{N} \beta_i \ell \left(\hat{\bvec{y}}_i, \bvec{y}_i\right)\]

        We can for instance use weights inversely proportional to the class frequency, such as $\beta_i = 1 - \frac{N_k}{N}$.
}


\section{Regression and classification}
\parag{Ridge regression}{
    The output of \important{ridge regression} is computed by a simple dot product: 
    \[\hat{\bvec{y}}_i = W^T \phi\left(\bvec{x}_i\right)\]

    The training objective function we want to minimise is a squared Euclidean distance regularised by the sum of squares of the weights:
    \[E\left(W\right) = R\left(W\right) + \lambda E_W \left(W\right) = \sum_{i=1}^{N} \left\|\hat{\bvec{y}}_i - \bvec{y}_i\right\|^2 + \lambda \left\|W\right\|^2_F\]
    where $\left\|W\right\|^2_F$ is the Frobenius norm of $W$, meaning the sum of the square of all its values, and $\lambda \geq 0$ is a hyperparameter.

    This can be solved explicitly: 
    \[W^* = \left(\Phi^T \Phi + \lambda I_F\right)^{-1} \Phi^T Y\]
    where $I_F$ is the $F \times F$ identity matrix.
    
}

\paragsubparag{}{Linear regression}{
        Leaving $\lambda = 0$, we get the special case of \important{linear regression}. Then, the closed-form formula can be rephrased as: 
        \[W^* = \left(\Phi^T \Phi\right)^{-1} \Phi^T \bvec{y} = \Phi^{\dagger} Y\]
        where $\Phi^{\dagger}$ is the Moore-Penrose pseudo-inverse.
}


\paragsubparag{}{Kernelisation}{

        Using the representer theorem, we can find that: 
        \[A^* = \left(K + \lambda I_N\right)^{-1} Y\]
        
        This is not of much use on its own, but we can use this result to find how we predict a value $\hat{\bvec{y}}$ for a new $\bvec{x}$: 
        \[\hat{\bvec{y}} = Y^T \left(K + \lambda I_N\right)^{-1} k\left(X, \bvec{x}\right)\]
        
        Note that the value $Y^T \left(K + \lambda I_N\right)^{-1}$ can be computed once during training, and then be reused at every evaluation of the model.
}


\paragsubparag{}{Classification}{

        Ridge regression can be used for classification tasks, by inputting the result into a softmax function (defined right after), but this does not work very well because we are not encoding this in the objective function.

        This is named a \important{least-square classifier}.
}



\parag{Logistic regression}{
    In \important{logistic regression} (which is a linear classification algorithm), we consider negative samples to be $y_i = 0$.

    The output of logistic regression is computed by using the \important{softmax function}: 
    \[\hat{y}^{\left(k\right)} = \frac{\exp\left(\bvec{w}^T_{\left(k\right)}  \bvec{x}\right)}{\sum_{j=1}^{C} \exp\left(\bvec{w}^T_{\left(j\right)} \bvec{x}\right)} \in \left[0, 1\right]\]
    where $\hat{y}^{\left(k\right)}$ represents the probability that the sample $\bvec{x}$ is in class $k$.

    The empirical risk we try to minimise is the \important{cross entropy}: 
    \[R\left(W\right) = - \sum_{i=1}^{N} \sum_{k=1}^{C} y_i^{\left(k\right)} \ln\left(\hat{y}_i^{\left(k\right)}\right)\]
    
    Note that $y_i^{\left(k\right)}$ is non-zero only for a single sample $i$.

    There is no closed-form formula, so we need the gradient of the empirical risk in order to do gradient descent (the solution is unique since the function is convex):
    \[\nabla_{W} R\left(W\right) = \sum_{i=1}^{N} \bvec{x}_i \left(\hat{\bvec{y}}_i - \bvec{y}_i\right)^T\]

}

\paragsubparag{}{One dimension}{
        Let's consider $C = 1$. This special case of the softmax function is named the \important{logistic function}:
        \[\hat{y}\left(\bvec{x}\right) = \sigma\left(\bvec{w}^T \bvec{x}\right) = \frac{1}{1 + \exp\left(- \bvec{w}^T \bvec{x}\right)}\]

        One-dimensional cross-entropy can be rewritten as: 
        \[R\left(\bvec{w}\right) = -\sum_{i \in P}^{} \ln\left(\hat{y}_i\left(\bvec{w}\right)\right) - \sum_{i \in N}^{} \ln\left(1 - \hat{y}_i\left(\bvec{w}\right)\right)\]
        where $P$ is the set of positive samples and $N$ the set of negative samples.

        The gradient of one-dimensional cross-entropy is: 
        \[\nabla_{\bvec{w}} R\left(\bvec{w}\right) = \sum_{i=1}^{N} \left(\hat{y}_i - y_i\right)\bvec{x}_i\]
}


\paragsubparag{}{Kernelisation}{

        This algorithm can be kernalised even though this is not very common.
}


\parag{Support vector machine}{
    In \important{support vector machine} (SVM) classification (which is also a linear classifier), we consider negative samples to be $y_i = -1$. Also, we leave $\widetilde{\bvec{w}}$ to be the vector of parameters without $w^{\left(0\right)}$, and $\bvec{x} \in \mathbb{R}^D$ to not have an added 1. Note that we only consider $C = 1$ for now.

    The idea of SVM is to maximise the size of the margin. A prediction is given by whether $w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i$ is closest to $-1$ or $1$ (as always for linear classifiers), and the decision boundary is given by $w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x} = \frac{-1 + 1}{2} = 0$. 

    We show in one of the following paragraphs that the problem can be formulated as:
    \[\argmin_{\bvec{w}, \left\{\xi_i\right\}} \frac{1}{2} \left\|\widetilde{\bvec{w}}\right\|^2 + C \sum_{i=1}^{N} \xi_i, \]
    subject to $y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right) \geq 1 - \xi_i$ and $\xi_i \geq 0$, for all $i$. Note that $C$ is an hyperparameter, and the $\xi_i$ are \important{slack variables} we added that we also minimise. Those slack variables allow for data which is not linearly separable data to also be used.

    When $\xi_i = 0$, the point is on the correct side of the margin, this is how everything should work. If $0 < \xi_i < 1$, then the point $i$ are on the wrong side of the margin, but correctly classified. If $\xi_i = 1$, the point is on the decision boundary (and thus misclassified). If $\xi_i > 1$, then the point is on the wrong side of the decision boundary, and thus misclassified.

    This representation of the problem is known as the \important{primal problem}.

}

\paragsubparag{}{Support vectors}{
        We notice that, for the margin to be maximised, there must be at least a point from each class lying on it. Such points are named \important{support vectors}. 
}


\paragsubparag{}{Hinge loss}{

        By rewriting the constraints, we get: 
        \[\xi_i \geq 1 - y_i\left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right)\]
        
        For samples $i$ that satisfy the support vector machine problem (they are on not in the margin nor misclassified), we have $\xi_i = 0$ (since they are forced to be non-negative). For samples $i$ which don't, this inequality is held. This allows us to rewrite our SVM primal problem as: 
        \[\argmin_{\bvec{w}, \left\{\xi_i\right\}} \frac{1}{2C} \left\|\widetilde{\bvec{w}}\right\|^2 + \sum_{i=1}^{N} \max\left(0, 1 - y_i\left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right)\right),\]
        subject to the same conditions
        
        This new term is called the \important{hinge loss}: 
        \[\max\left(0, 1 - y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right)\right)\]
}


\paragsubparag{}{Dual problem}{

        We can reformulate our problem by letting one variable per training sample (meaning that we have $N$ variables instead of $\left(D+1\right)$): 
        \[\argmax_{\left\{\alpha_i\right\}} \left(\sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \bvec{x}_i^T \bvec{x}_j\right),\]
        subject to $\sum_{i=1}^{N} \alpha_i y_i = 0$ and $0 \leq \alpha_i \leq C$ for all $i$.
        
        The solution is equivalent to the primal problem: 
        \[\widetilde{\bvec{w}}^* = \sum_{i=1}^{N} \alpha_i^* y_i \bvec{x}_i \implies \hat{y}\left(\bvec{x}\right) = w^{\left(0\right)*} + \sum_{i = 1}^{N} \alpha_i y_i \bvec{x}_i^T \bvec{x}\]
        
        Note that $w^{\left(0\right)}$ can also be found thanks to those $\alpha_i^*$.

        We can show that, at the solution, we have: 
        \[\alpha_i^* \left(y_i \left(w^{\left(0\right)*} + \widetilde{\bvec{w}}^{*T} \bvec{x}_i\right) - 1 + \xi_i^*\right) = 0\]

        In other words, for every sample, either of those terms is equal to 0. The samples for which $\alpha_i^* \neq 0$ are the support vectors. However, most samples are not support vectors, so we can decrease the computations by leaving $\mathcal{S}$ to be the set of support vectors: 
        \[\hat{y}\left(\bvec{x}\right) = w^{\left(0\right)*} + \sum_{i \in\mathcal{S}}^{} \alpha_i^* y_i \bvec{x}_i^T \bvec{x}_i\]
}


\paragsubparag{}{Kernelisation}{
     
        We need the dual problem to kernelise the SVM: 
        \[\argmax_{\left\{\alpha_i\right\}} \left(\sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j k\left(\bvec{x}_i, \bvec{x}_j\right)\right),\]
        subject to $\sum_{i=1}^{N} \alpha_i y_i = 0$ and $0 \leq \alpha_i \leq C$ for all $i$.

        The prediction is also similar: 
        \[\hat{y}\left(\bvec{x}\right) = w^{\left(0\right)*} + \sum_{i \in \mathcal{S}}^{} \alpha_i^* y_i k\left(\bvec{x}_i, \bvec{x}\right)\]

        Note that we still have $\alpha_i^* = 0$ for all samples that are not support vectors.
}


\paragsubparag{}{Multi-class SVM}{
    
        To generalise our algorithm to multiple class, we can use multiple ways. The idea is always to use several binary classifiers.

        One way is to use \important{one-vs-rest}: we train classifiers stating if the component is in class $i$ or not. Another way is to use \important{one-vs-one}: we train classifiers to know if the component is closer to class $i$ or $j$, and then pick the best one. However, in both cases, there are some samples which will give ambiguous answers (belonging to multiple classes or to none).
}


\paragsubparag{}{Primal derivation}{
    Let's consider how we got the formula for the primal model, since it may typically help to remember and understand it.

    First, we know that any two points on the decision boundary have the same prediction (which is 0), which yields that: 
    \[0 = \hat{y}_1 - \hat{y}_2 = \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_1\right) - \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_2\right) = \widetilde{\bvec{w}}^T \left(\bvec{x}_1 - \bvec{x}_2\right)\]

    This is a dot product, and it thus means that $\widetilde{\bvec{w}}$ is orthogonal to the decision boundary. 

    Second, we use this information to split any point into a component colinear to $\widetilde{\bvec{w}}$, and one orthogonal to it (meaning colinear to the decision boundary): 
    \[\bvec{x} = \bvec{x}_{\perp} + r \frac{\widetilde{\bvec{w}}}{\left\|\widetilde{\bvec{w}}\right\|}\]
    where $r$ is the signed orthogonal distance of any point to the decision boundary.

    Now, looking at the prediction yielded by this point, we get: 
    \[\hat{y}\left(\bvec{x}\right) = w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x} = w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_{\perp} + r \frac{\widetilde{\bvec{w}}^T \widetilde{\bvec{w}}}{\left\|\widetilde{\bvec{w}}\right\|} = y\left(\bvec{x}_{\perp}\right) + r\left\|\widetilde{\bvec{w}}\right\|\]

    However, we know that $y\left(\bvec{x}_{\perp}\right) = 0$ since it is on the decision boundary, meaning that: 
    \[\hat{y}\left(\bvec{x}\right) = r\left\|\widetilde{\bvec{w}}\right\| \iff r = \frac{\hat{y}\left(\bvec{x}\right)}{\left\|\widetilde{\bvec{w}}\right\|}\]
    which is, to recall, the signed orthogonal distance of $\bvec{x}$ to the decision boundary.

    Note that we can then make use of the ground truth label being $-1$ or 1 to make an unsigned distance: 
    \[\widetilde{r}_i = y_i r_i = \frac{y_i\left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right)}{\left\|\widetilde{\bvec{w}}\right\|}\]

    This is what we would like to maximise, but it is hard. We thus need to turn it to an equivalent problem. To do so, we notice that there is an infinite number of solutions that matter, since we only want the direction of $\bvec{w}$ and that its magnitude does not matters. This can be proven mathematically by multiplying our weights by a $\lambda$, and seeing that we get the same $\widetilde{r}_i$ above (the $\lambda$ cancel out in the fraction).

    So, we may as well require that the margin has size $\frac{1}{\left\|\widetilde{\bvec{w}}\right\|}$, meaning that: 
    \[r_i \geq \frac{1}{\left\|\widetilde{\bvec{w}}\right\|} \iff r_i \left\|\widetilde{\bvec{w}}\right\| \geq 1 \iff y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right) \geq 1\]

    Now, maximising the margin means maximising $\frac{1}{\left\|\widetilde{\bvec{w}}\right\|}$ which can be shown to be equivalent to minimising $\left\|\widetilde{\bvec{w}}\right\|^2$. Our problem has thus become: 
    \[\argmin_{\bvec{w}} \frac{1}{2} \left\|\widetilde{\bvec{w}}\right\|^2, \mathspace \text{subject to } y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right) \geq 1, \ \forall i\]
    where the factor $\frac{1}{2}$ was added for convenience.

    However, if the data is not linearly separable, we need a way to let some samples violate this rule. This is done by adding \important{slack variables} $\xi_i \geq 0$ for each sample: 
    \[y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}} \bvec{x}_i\right) \geq 1 - \xi_i\]

    In other words, if $0 < \xi_i \leq 1$, the sample $i$ lies inside in the margin but is still correctly classified. If $\xi_i \geq 1$, then the sample $i$ is misclassified. 

    We minimise those variables jointly with our original problem, giving us our final formulation:
    \[\argmin_{\bvec{w}, \left\{\xi_i\right\}} \frac{1}{2} \left\|\widetilde{\bvec{w}}\right\|^2 + C \sum_{i=1}^{N} \xi_i, \]
    subject to $y_i \left(w^{\left(0\right)} + \widetilde{\bvec{w}}^T \bvec{x}_i\right) \geq 1 - \xi_i$ and $\xi_i \geq 0$, for all $i$.
}

\parag{$K$-nearest neighbours}{
    The idea of \important{$k$-nearest neighbours} (kNN) is to compute the distance between the test sample $\bvec{x}$ and all training samples $\left\{\bvec{x}_i\right\}$ and find the $k$ samples with minimum distances. Then, we can do classification by finding the most common label amongst these $k$ nearest neighbours, or do regression by computing $\hat{\bvec{y}}$ as a function of those neighbours and their distance to $\bvec{x}$.

    To compute close points efficiently we can use datastructures such as $k$-d trees.

}

\paragsubparag{}{Remark}{
        The result of this model depends on the choice of the distance function. One can take the \important{Euclidean distance}: 
        \[d\left(\bvec{x}_i, \bvec{x}\right) = \sqrt{\sum_{d=1}^{D} \left(x_i^{\left(d\right)} - x^{\left(d\right)}\right)^2}\]
        
        However, for some other structures such as histograms (each data point only has component between 0 and 1, and the sum of all of the components of a data point is equal to 1; like for a probability distribution), then we instead tend to use a Chi-square distance: 
        \[d\left(\bvec{x}_i, \bvec{x}\right) = \sqrt{\chi^2 \left(\bvec{x}_i, \bvec{x}\right)} = \sqrt{\sum_{d=1}^{D} \frac{\left(x_i^{\left(d\right)} - x^{\left(d\right)}\right)^2}{x_i^{\left(d\right)} + x^{\left(d\right)}}} \]
        
        The important thing to remember from this paragraph is that the choice of the distance function is important.
}


\paragsubparag{}{Curse of dimensionality}{

        Because of a principle named the \important{curse of dimensionality}, we need exponentially more points to cover a space as the number of dimensions increases. Using dimensionality reduction is a good idea with this algorithm.
}


\paragsubparag{}{Complexity}{
    

        Unlike most of the other models, increasing the hyperparameter of this model (the $k$) leads to decreased complexity: the higher the $k$, the less complex the decision boundary is and thus the less overfit we have.
}


\parag{Neural networks}{
    Neural networks can do both classification and regression (depending on the output representation and the empirical risk used, typically square loss for regression and cross-entropy for classification), and their main advantage is that they learn a good model during the training.

    This method is named \important{neural network}, \important{multi-layer perceptron} (MLP), or \important{deep learning} (as long as there are at least two hidden layers).

    The idea is to have layers composed of neurons. Every neuron of a layer is connected to every neurons of the previous layer, meaning that its value is computed by a weighted sum over them, plus a bias. More than that, the important thing for our model not to be just a big linear regression, is that each neuron is passed through a non-linear activation function. Mathematically speaking, this is given by: 
    \[\bvec{z}_{\left(l \right)} = f_{\left(l \right)} \left(W_{\left(l \right)}^T \bvec{z}_{\left(l - 1\right)}\right)\]
    where $\bvec{z}_{\left(0\right)} = \bvec{x}$ is the input layer, $\bvec{z}_{\left(L+1\right)} = \hat{\bvec{y}}$ is the output layer, $L$ is the number of hidden layers (layers which are neither input nor output), and $f_{\left(l\right)} \left(x\right)$ is applied to every component of the vector. Note that each $\bvec{z}_{\ell }$ has a ``bias term'' just like the input data (meaning a 1 appended at the beginning).

    This is trained to optimality using stochastic gradient descent, by focusing on a single loss term $\ell \left(\hat{\bvec{y}}_i, \bvec{y}_i\right)$ at a time. To do so, we need to compute the gradients $\frac{\partial \ell_i}{\partial W_l^{\left(k, j\right)}}$ (where we are considering the loss of the $i$\Th sample, and the weight at position $\left(k, j\right)$ of the weight matrix from layer $l$). This is done by an algorithm named \important{backpropagation}.

    We can notice that, by the chain rule (and abusing slightly of the notation of the derivative): 
    \[\frac{\partial \ell _i}{\partial W_l} = \frac{\partial z_{\left(l\right)}}{\partial W_{l}} \frac{\partial z_{\left(l+1\right)}}{\partial z_l}  \cdots \frac{\partial z_{\left(L\right)}}{\partial z_{\left(L-1\right)}} \frac{\partial \ell _i}{\partial z_{\left(L\right)}} \]
    
    We can compute $\frac{\partial z_{\left(l\right)}}{\partial z_{\left(l-1\right)}} $ and $\frac{\partial z_{\left(l\right)}}{\partial W_l} $ rather easily if we store the values of each layer during the forward pass. We can then propagate backwards those value, by computing the gradients from the end and updating the weights.

}

\paragsubparag{}{Activation functions}{
        There are multiple choice for activation functions. The important thing is that they are non-linear.

        We can for instance take the ReLU (Rectified Linear Unit) activation function: 
        \begin{functionbypart}{f\left(a\right)}
            a, & \text{if } a > 0 \\
            0, & \text{otherwise}
        \end{functionbypart}
        
        Another choice could be the sigmoid: 
        \[f\left(a\right) = \frac{1}{1 + \exp\left(-a\right)}\]
}


\paragsubparag{}{Convolutional network}{

        When working with pictures, just vectorising them may give a lot of elements while removing the fact that the picture is inherently two-dimensional. A way to circumvent this problem is using convolutions.

        To make a convolution, we need a small matrix of elements (plus a bias). We center this matrix at an element, compute the weighted sum resulting from it, add the bias, and use this as our result. We can then shift it to center it on each element, yielding a new picture. This allows to extract some features of our original picture, such as the edges.

        We can also use multiple filters to create multiple channels, increasing the amount of data. If at some point we have 3 channels (for instance) and want to do a convolution with a $5\times5$ filter, then we will use matrices of size $5\times 5 \times 3$ (going three-dimensional over our channels).  Note that we can also use some other parameters, such as strides (skip one pixels over two for instance) or padding (add some pixels on the edges).

        We can also use pooling layers, splitting the pixels into $k \times k$ squares and extracting only one value per partition (by taking the maximum value or the average for instance). This allows to decrease the size of our pictures.

        The main interest then comes from stacking those operations. For instance, beginning with a $28 \times 28$ input, we may apply convolutions to get a $3 \times 24 \times 24$ layer (we loose some pixels because we may require the filter to ignore pixels where it has to be partly outside of the picture) and then a pooling layer to get a $3 \times 12 \times12$ layer.

        The idea of a \important{convolutional neural network} (CNN) is to make some convolutions (typically before the input layers, but also in-between some layers). The main idea is that we also optimise the values inside the filters of our convolutions. To compute the gradient for them, we also use back-propagation.
}


\section{Dimensionality reduction and clustering}

\parag{Principle component analysis}{
    Sometimes, we realise that data is given in many dimensions, but actually lies in many less dimensions. The idea of \important{principle component analysis} (PCA) is to project the data on some orthogonal axis (of lower dimension), in a way to maximise the kept variance.

    Leaving $\bar{\bvec{x}} = \frac{1}{N}\sum_{i=1}^{N} \bvec{x}_i$ to be the mean, we have: 
    \[\bvec{y}_i = W^T \left(\bvec{x}_i - \bar{\bvec{x}}\right)\]

    To find this matrix $W \in \mathbb{R}^{D \times d}$ (where $d$ is the number of dimensions after the projection), we first need to consider the data covariance matrix: 
    \[C = \frac{1}{N} \sum_{i=1}^{N} \left(\bvec{x}_i - \bar{\bvec{x}}\right) \left(\bvec{x}_i - \bar{\bvec{x}}\right)^T\]
    
    Then, picking the $d$ eigenvectors which highest eigenvalues of $C$, we get our matrix: 
    \[W = \begin{pmatrix}  &  &  \\ \bvec{w}_{\left(1\right)} & \cdots & \bvec{w}_{\left(d\right)} \\  &  &  \end{pmatrix} \in \mathbb{R}^{D \times d}\]

    The explained variance yielded by our projection can be found by computing: 
    \[\text{exval} = \frac{\sum_{j=1}^{d} \lambda_j}{\sum_{j=1}^{D} \lambda_j}\]

    Using PCA can make the computations of the models much faster without losing much precision, or getting some insight of the data.
    
}

\paragsubparag{}{Remark}{
        Since the axis on which we project the data are orthogonal, we have: 
        \[W^T W = I_d\]

        To make sure of this, we need to take the eigenvector so that they are orthogonal. This can always be done because $C$ is symmetric, thanks to the spectral theorem (this theorem also allows to know that we can compare eigenvalues, since they are real).
}


\paragsubparag{}{Mapping}{

        From our computation, we can notice that, for any point $\bvec{y} \in\mathbb{R}^d$, we can move it to the high-dimensional space: 
        \[\hat{\bvec{x}} = \bar{\bvec{x}} + W \bvec{y}\]

        This yields all the advantages presented in the first section of this document.
}


\paragsubparag{}{Kerenelisation}{

        PCA can be kernalised in a non-trivial fashion.

        First, we need to account for the fact that our data may not be centered in input-space (this was done above by considering the mean of our input values), letting: 
        \[\widetilde{K} = K - 1_N K - K 1_N + 1_N K 1_N\]
        where $1_N$ is an $N \times N$ matrix, with every element equal to $\frac{1}{N}$.

        Leaving $\bvec{a}$ to be the vector of unknowns given by the representer theorem, we can find that it follows the following equation: 
        \[\widetilde{K} \bvec{a} = \lambda N \bvec{a}\]
        
        This is an eigenvalue problem, which could be solved to find a solution $\bvec{a}$. From there, we can project our data: 
        \[y_i = \sum_{j=1}^{N} a_j k\left(\bvec{x}_i, \bvec{x}_j\right)\]
        supposing that we want $d = 1$.

        If we want $d > 1$, we can again take the $d$ eigenvectors with greatest eigenvalues, and compute each component of $\bvec{y}_i$ thanks to a different eigenvector.

        Note that we can no longer map data from $d$ dimensions back to $D$ dimensions with the kernalised method (since it would require us to know the feature expansion mapping $\phi\left(\bvec{x}\right)$).
}


\parag{Autoencoder}{
    Another way to do dimensionality reduction is through a neural network.

    The idea is to have a double funnel shaped neural network: an encoder decreasing the dimension, a layer with $d$ neurons, and a decoder increasing back the number of dimensions.
    \imagehere[0.7]{Autoencoder.png}

    We can train it to output the same data as what we input it, using a least square empirical risk. That way, it must learn an intelligent code.

}

\paragsubparag{}{Remark}{
        This can also be used to both do dimensionality reduction and mapping data back from the low-dimensional space.
}


\paragsubparag{}{Convolutional autoencoder}{
    
        We can use convolutional neural networks for autoencoders. To do so, we use the inverse functions of those convolutions.
}


\parag{Fisher linear discriminant analysis}{
    Even though \important{Fisher linear discriminant analysis} (LDA) is a dimensionality reduction algorithm, it is a supervised learning one. Its goal is to project data on lower space, while keeping classes (hence the supervision) clustered. It considers that clustering should be done relatively to compactness, meaning distance between points within a cluster should be small, whereas distances across clusters should be large.

    The goal is to minimise the distances within a class, meaning the distance of the elements of a class to their class center $E_W\left(\bvec{w}\right)$, while maximising the distance of cluster centers (weighted by the number of elements in the class) $E_B\left(\bvec{w}\right)$. This is better expressed thanks to the within-class scatter matrix $S_W$ and the between-class scatter matrix $S_B$: 
    \[S_W = \sum_{c=1}^{C} \sum_{i \in c}^{} \left(\bvec{x}_i - \bvec{\mu}_c\right) \left(\bvec{x}_i - \bvec{\mu}_c\right)^T, \mathspace S_B = \sum_{c=1}^{C} N_c \left(\bvec{\mu}_c - \bar{\bvec{x}}\right)\left(\bvec{\mu}_c - \bar{\bvec{x}}\right)^T\]
    where $\bar{\bvec{x}}$ si the mean of all the samples, and $\bvec{\mu}_c$ the mean of the values in class $c$.

    Our problem can be specified as the following generalised eigenvector problem:
    \[S_B \bvec{w}_{\left(1\right)} = \lambda_1 S_w \bvec{w}_{\left(1\right)}\]
    
    As for PCA we are in fact looking for the $d$ eigenvectors with largest eigenvalues. 
}

\parag{$K$-means clustering}{
    The idea of \important{$k$-means clustering} is to consider that clustering should also be done relatively to compactness.

    To do so, we consider $K$ (an hyperparameter) cluster centers $\left\{\bvec{\mu}_1, \ldots, \bvec{\mu}_K\right\}$. While we are not converged, we assign each data point $\bvec{x}_i$ to the nearest center $\bvec{\mu}_k$, and then move  $\bvec{\mu}_k$ to the mean of the points that were assigned to it.

    This algorithm is guaranteed to converge, even though it may not be to the expected solution: some clusters may end up completely empty. A good way to fix this problem is to run it multiple times with different initialisation, and pick the solution minimising the sum of the distance between each point and their assigned cluster center.

}

\paragsubparag{}{Hyperparameter}{
        To choose the $K$, a good way is to draw a graph of the average within-cluster sum of distances with respect to the number of cluster, and pick a point at its ``elbow'' (where the drop in the $y$-axis becomes less significant).
}


\parag{Spectral clustering}{
    The idea of \important{spectral clustering} is to consider that clustering should be done relatively to connectivity instead: we group the points based on edges in a graph, and remove some of the edges with longest length. Let us first consider the case where we only want to make 2 clusters, meaning that we only want one cut.

    To create the graph, we need a way to give weights to edges in order to represent their affinity, a way to do so is: 
    \[w\left(i, j\right) = \exp\left(\frac{-\left\|\bvec{x}_i - \bvec{x}_j\right\|^2}{\sigma ^2}\right)\]
    where $\sigma$ is an hyper-parameter. Note that this weight decreases as the distance between $\bvec{x}_i$ and $\bvec{x}_j$ increases. Also, considering this full graph may be expensive, so we can also restrict to the $k$ nearest neighbours of each points.

    The goal is now to find a partition $\left\{A, B\right\}$, minimising the sum of weights of the edges we have to remove. We thus let this value to be named the cut: 
    \[\text{cut}\left(A, B\right) = \sum_{i \in A}^{} \sum_{j \in B}^{} w\left(i, j\right)\]
    
    Just minimising the cut might favour imbalanced partitions, so we also define the degree of a node in the graph $d_i$ and the volume of a partition to be given by: 
    \[d_i = \sum_{j}^{} w\left(i, j\right), \mathspace \text{vol}\left(A\right) = \sum_{i \in A}^{} d_i\]
    
    Our goal is now to minimise a normalised cut: 
    \[\text{ncut}\left(A, B\right) = \frac{\text{cut}\left(A, B\right)}{\text{vol}\left(A\right)} + \frac{\text{cut}\left(A, B\right)}{\text{vol}\left(B\right)}\]

    This problem is NP-hard, but it can be relaxed as the following eigenvector problem: 
    \[\left(D - W\right)\bvec{y} = \lambda D \bvec{y}\]
    where $D$ is the diagonal degree matrix ($D_{i, i} = d_i$) and $W$ is the affinity matrix of the graph ($W_{i, j} = w\left(i, j\right)$).
    
    The eigenvector with smallest eigenvalue can be shown to be a vector of all ones with eigenvalue 0. We are thus looking for the eigenvector with second smallest eigenvalue. A positive value in this vector indicates that the corresponding point belongs to one partition, and a negative value to the other.

}

\paragsubparag{}{Remark}{
        Since we had to relax the problem, the solution is not always optimal.
}


\paragsubparag{}{$K$-way partitions}{
    
        To obtain more than two clusters, we have two choices.

        The first one is to recursively apply the two-way partitioning algorithm. This is inefficient and unstable.

        The second one is to find $K$ eigenvectors. This leads to each point being represented by a $K$-dimensional vector. We can then apply $K$-means clustering to those resulting vectors.
}


\end{document}

