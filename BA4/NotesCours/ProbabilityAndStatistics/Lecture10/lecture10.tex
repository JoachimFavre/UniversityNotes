% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use Kuala Tex (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2023-03-23 at 13:20:43.

\usepackage{../../style}

\title{Probability and statistics}
\author{Joachim Favre}
\date{Jeudi 23 mars 2023}

\begin{document}
\maketitle

\lecture{10}{2023-03-23}{Transformations}{
\begin{itemize}[left=0pt]
    \item Definition of uniform, exponential, gamma, Laplace and Pareto random variables.
    \item Definition of expectation, variance and conditional densities for continuous random variables.
    \item Definition of quantile.
    \item Example of transformations.
\end{itemize}

}

\parag{Remark}{
    We notice that it is possible to have $f\left(x\right) > 1$ for some $x$, as long as $\int_{-\infty}^{\infty} f\left(x\right)dx = 1$. It might just be spiky.

    The important thing to consider is that $f$ is not a probability, but a density: it is area under its curve which represent probabilities.
}

\parag{Definition: Uniform distribution}{
    Let $a < b$. A \important{uniform random variable} $U$ is a random variable with density:
    \begin{functionbypart}{f\left(u\right)}
        \frac{1}{b-a}, & a \leq u \leq b, \\
        0, & \text{otherwise}
    \end{functionbypart}

    We write $U \followsdistr U\left(a, b\right)$.

    \subparag{CDF}{
        We can compute the CDF of this function: 
        \begin{functionbypart}{F\left(u\right) = \int_{-\infty}^{u} f\left(x\right)du}
            0, & u \leq a \\
            \frac{u-a}{b-a}, & a < u \leq b \\
            1, & u > b
        \end{functionbypart}
    }
}

\parag{Definition: Exponential distribution}{
    Let $\lambda > 0$. An \important{exponential random variable} is a random variable $X$ with density:
    \begin{functionbypart}{f\left(x\right)}
        \lambda e^{-\lambda x}, & x > 0 \\
        0, & \text{otherwise}
    \end{functionbypart}

    We write $X \followsdistr \exp\left(\lambda\right)$.

    \subparag{CDF}{
        We can integrate this function to get its CDF:
        \begin{functionbypart}{F\left(x\right)}
            0, & x \leq 0 \\
            1 - \exp\left(-\lambda x\right), & x > 0 \\
        \end{functionbypart}
    }
    
}

\parag{Theorem: Lack of memory}{
    An exponential distribution has the lack of memory property: 
    \[\prob\left(X > x + t | X > t\right) = \prob\left(X > x\right), \mathspace t, x > 0\]
    
    \subparag{Proof}{
        We can use the CDF of the exponential distribution: 
        \autoeq{\prob\left(X > x + t | X > t\right) = \frac{\prob\left(X > x + t\right)}{\prob\left(X > t\right)} = \frac{\exp\left(-\lambda\left(x + t\right)\right)}{\exp\left(-\lambda t\right)} = \exp\left(-\lambda x\right) = \prob\left(X > x\right)}

        \qed
    }
}

\parag{Definition: Gamma function}{
    The \important{Gamma function}, $\Gamma : \mathbb{R}_+ \mapsto \mathbb{R}$ is defined as: 
    \[\Gamma\left(\alpha\right) = \int_{0}^{\infty} u^{\alpha - 1} e^{-u} du, \mathspace \alpha > 0\]

    \subparag{Properties}{
        This is a generalisation of the factorial to all positive (and in fact, complex) numbers since it follows the following recurrence relation: 
        \[\Gamma\left(1\right) = 1, \mathspace \Gamma\left(\alpha + 1\right) = \alpha \Gamma\left(\alpha\right), \mathspace \alpha > 0\]

        This thus yields that: 
        \[\Gamma\left(n\right) = \left(n-1\right)!, \mathspace n \in \mathbb{N}^*\]
        
        We for instance have that: 
        \[\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}\]
    }

    \subparag{Remark}{
        This function is not important to remember.
    }
}


\parag{Definition: Gamma distribution}{
    Let $\alpha, \lambda > 0$. A \important{gamma random variable} $X$ has density: 
    \begin{functionbypart}{f\left(x\right)}
        0, & x \leq 0  \\
        \frac{\lambda^{\alpha}}{\Gamma\left(\alpha\right)} x^{\alpha - 1} e^{-\lambda x}, & x > 0 \\
    \end{functionbypart}
}

\parag{Definition: Laplace distribution}{
    Let $\epsilon \in \mathbb{R}, \lambda > 0$. A \important{Laplace random variable} (or double exponential) $X$ has density: 
    \[f\left(x\right) = \frac{\lambda}{2} e^{-\lambda \left|x - \epsilon\right|}, \mathspace x \in \mathbb{R}\]

    \subparag{CDF}{
        We have the following CDF:
        \begin{functionbypart}{F\left(x\right)}
            \frac{1}{2} e^{-\lambda \left|x - \epsilon\right|}, & x \leq \epsilon \\
            1 - \frac{1}{2} e^{-\lambda \left|x - \epsilon\right|}, & x > \epsilon
        \end{functionbypart}
    }
}

\parag{Definition: Pareto distribution}{
    Let $\alpha, \beta > 0$. A \important{Pareto random variable} has the following distribution: 
    \begin{functionbypart}{f\left(x\right)}
        0, & x < \beta \\
        \frac{\alpha \beta^{\alpha}}{x^{\alpha + 1}}, & x \geq \beta
    \end{functionbypart}
    
    \subparag{CDF}{
        It has the following CDF: 
        \begin{functionbypart}{F\left(x\right)}
            0, & x < \beta \\
            1 - \left(\frac{\beta}{x}\right)^{\alpha}, & x \geq \beta
        \end{functionbypart}
        
    }
}

\parag{Definition: Expectation}{
    Let $g\left(x\right)$ be a real-valued function, and $X$ a continuous random variable of density $f\left(x\right)$. Then, if $\exval\left(\left|g\left(x\right)\right|\right) < \infty$, we define the \important{expectation} of $g\left(X\right)$ to be: 
    \[\exval\left[g\left(X\right)\right] = \int_{-\infty}^{\infty} g\left(x\right)f\left(x\right)dx\]
    
    \subparag{Intuition}{
        This is almost the same as for the discrete case, except that we replace sums by integrals.
    }
    
    \subparag{Expectation and variance}{
        In particular, the \important{expectation} of $X$ is: 
        \[\exval\left(X\right) = \int_{-\infty}^{\infty} xf\left(x\right) dx\]
    }
}

\parag{Definition: Variance}{
    Let $X$ be some random variable such that the following integral exists. Then, its \important{variance} is:
        \[\Var\left(X\right) = \int_{-\infty}^{\infty} \left[x - \exval\left(X\right)\right]^2 f\left(x\right)dx = \exval\left(X^2\right) - \exval\left(X\right)^2\]
}


\parag{Example}{
    We want to compute the expectation and the variance of $X \followsdistr U\left(a, b\right)$. 

    Let us first compute: 
    \[\exval\left(X^r\right) = \int_{-\infty}^{\infty} x^r \frac{1}{b-a}dx = \frac{b^{r+1} - a^{r+1}}{\left(r+1\right)\left(b-a\right)}\]

    This gives us the expectation: 
    \[\exval\left(X\right) = \frac{b^2 - a ^2}{2\left(b-a\right)} = \frac{b+a}{2}\]
    which is the middle point, very intuitively.
    
    We can then compute the variance: 
    \[\Var\left(X\right) = \exval\left(X^2\right) - \exval\left(X\right)^2 = \frac{\left(b-a\right)^2}{12}\]
}

\parag{Definition: Conditional densities}{
    Let $\mathcal{A} \subset \mathbb{R}$ be a reasonable subset, and:
    \[\mathcal{A}_x = \left\{y \suchthat y \leq x \text{ and } y \in \mathcal{A}\right\}\]

    Then, the conditional PDF is defined as: 
    \begin{functionbypart}{f_X\left(x | X \in \mathcal{A}\right)}
        \frac{f_X\left(x\right)}{\prob\left(X \in \mathcal{A}\right)}, & x \in \mathcal{A} \\
        0, & \text{otherwise}
    \end{functionbypart}
    
    The CDF is given by: 
    \[F_X\left(x | X \in \mathcal{A}\right) = \prob\left(X \leq x | X \in \mathcal{A}\right) = \frac{\prob\left(X \leq x \cap X \in\mathcal{A}\right)}{\prob\left(X \in \mathcal{A}\right)} = \frac{1}{\prob\left(X \in A\right)}\int_{A_x} f\left(y\right)dy\]
    
    Finally, we define the expectancy:
    \[\exval\left[g\left(X\right) | X \in \mathcal{A}\right] = \frac{\exval\left[g\left(X\right)I\left(X \in \mathcal{A}\right)\right]}{\prob\left(X \in \mathcal{A}\right)}\]
    where $I$ is an indicator variable.
}

\parag{Definition: Quantile}{
    Let $0 < p < 1$. We define the $p$ quantile of the cumulative distribution function $F\left(x\right)$ to be: 
    \[x_p = \inf\left\{x \suchthat F\left(x\right) \geq p\right\}\]

    The infimum is required when the function is discontinuous, or when it is flat for some values.
    
    \subparag{Intuition}{
        For most continuous random variables, $x_p$ is unique and is given by $x_p = F^{-1}\left(p\right)$.
    }
}

\parag{Definition: Median}{
    The \important{median} of some cumulative distribution function is the $\frac{1}{2}$ quantile.

    \subparag{Intuition}{
        It represents the point where we have $\SI{50}{\%}$ chance to be above, and $\SI{50}{\percent}$ chance to be below.
    }
}

\parag{Example}{
    Let $X \followsdistr \exp\left(\lambda\right)$. We want to find the $p$ quantile of $X$.

    We have to solve $F\left(x_p\right) = p$: 
    \[1 - \exp\left(-\lambda x_p\right) = p \iff x_p = -\lambda^{-1} \log\left(1 - p\right)\]
}

\subsection{Transformations}
\parag{Example}{
    Let $W = -\log\left(U\right)$, where $U \followsdistr U\left(0, 1\right)$. We want to find $F_W\left(w\right)$.

    We notice that, for some $w > 0$: 
    \autoeq{\prob\left(W \leq w\right) = \prob\left[-\log\left(U\right) \leq w\right] = \prob\left(U \geq \exp\left(-w\right)\right) = 1 - \prob\left(U < e^{-w}\right) = 1 - \prob\left(U \leq e^{-w}\right) = 1 - e^{-w}}
    which is our result.

    Note that we have $\prob\left(U < e^{-w}\right) = \prob\left(U \leq e^{-w}\right)$ since the probability of any single point is 0.
}

\parag{Definition: Inverse function}{
    Let $g: \mathbb{R}\mapsto \mathbb{R}$ be a function, and $\mathcal{B} \subset \mathbb{R}$ any subset of $\mathbb{R}$. 

    We write $g^{-1}\left(\mathcal{B}\right) \subset \mathbb{R}$ to be the set for which $g\left[g^{-1}\left(\mathcal{B}\right)\right] = \mathcal{B}$
}

\parag{Remark}{
    We have access to some $U \followsdistr U\left(0, 1\right)$. Given any arbitrary CDF $F_*$, we want to find a transformation $g: \mathbb{R}\mapsto \mathbb{R}$ such that, letting $Y = g\left(U\right)$: 
    \[F_Y\left(y\right) = F_*\left(y\right)\]

   In other words, we want: 
   \[\prob\left(g\left(U\right) \leq x\right) = F_*\left(x\right)\]
   
   To do so, we can see that $g = F_*^{-1}$ works: 
   \[\prob\left(g\left(U\right) \leq x\right) = \prob\left(F_*^{-1}\left(U\right) \leq x\right) = \prob\left(U \leq F_*\left(x\right)\right) = F_*\left(X\right)\]

   This result is very general. It means that if we have access to some uniform random generator and we want to generate numbers following any arbitrary PDF $f_*\left(x\right)$, we only need to integrate the PDF to get the CDF $F_*\left(x\right)$, inverse it, and pass any sample $u$ into this $F_*^{-1}\left(x\right)$.
   
    \subparag{Personal note: Usage}{
        Such transformations can be very important. For instance, in computing, we have pseudo-random number generators which give some uniform distributions. We may then want to map those to another distribution for different use cases.

        For example, in Computer Graphics, we need to uniformly sample points on a hemisphere. A good idea is to sample points using polar coordinates. However, using this technique blindly with a uniform distribution does not yield a uniform distribution on the sphere (there will be more points near the poles). We thus need to map our uniform points to some other distributions.
    }
}

\end{document}
