% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2023-05-25 at 13:22:29.

\usepackage{../../style}

\title{Probastats}
\author{Joachim Favre}
\date{Jeudi 25 mai 2023}

\begin{document}
\maketitle

\lecture{24}{2023-05-25}{Finally, the null hypotheses}{
\begin{itemize}[left=0pt]
    \item Definition of the standard error, and proof of the empirical CLT.
    \item Definition of null and alternative hypothesis.
    \item Definition of false positive and false negative.
    \item Definition of the Chi-square distribution.
    \item Definition of Pearson statistic, and proof of its link with the Chi-square distribution.
\end{itemize}

}

\parag{Example 3}{
    Let us consider again $Y_1, \ldots, Y_n \followsdistr U\left(0, \theta\right)$. We have already seen that the following random variable is an approximate pivot: 
    \[Q = \frac{\bar{y} - \frac{\theta}{2}}{\sqrt{\frac{\theta^2}{12 n}}} \over{\to}{D} \mathcal{N}\left(0, 1\right)\]

    We want to find a random interval with symmetric confidence level $\frac{\alpha}{2}$. The quantiles of a normal distribution can be computed numerically and, by symmetry, $z_{\frac{\alpha}{2}} = -z_{1 - \frac{\alpha}{2}}$. Since the second is positive for $\alpha < 0.5$, we prefer it by simplicity for inequalities. We thus have: 
    \[\prob\left(-z_{1 - \frac{\alpha}{2}} \leq Q \leq z_{1 - \frac{\alpha}{2}}\right) \to 1 - \alpha\]
    
    Now, considering the inequality inside the probability: 
    \autoeq{-z_{1 - \frac{\alpha}{2}} \leq \frac{\bar{y} - \frac{\theta}{2}}{\sqrt{\frac{\theta^2}{12n}}} \leq z_{1 - \frac{\alpha}{2}} \iff -z_{1 - \frac{\alpha}{2}} \leq \frac{\frac{\bar{y}}{\theta} - \frac{1}{2}}{\frac{1}{\sqrt{12n}}} \leq z_{1 - \frac{\alpha}{2}} \iff \frac{1}{2} - \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12n}} \leq \frac{\bar{y}}{\theta} \leq \frac{1}{2} + \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12n}}\iff \frac{\bar{y}}{\frac{1}{2} + \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12n}}} \leq \theta \leq \frac{\bar{y}}{\frac{1}{2} - \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12n}}}}

    This yields that we can take: 
    \[L = \frac{\bar{Y}}{\frac{1}{2} + \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12 n}}}, \mathspace U = \frac{\bar{Y}}{\frac{1}{2} - \frac{z_{1 - \frac{\alpha}{2}}}{\sqrt{12 n}}}\]
    
    Giving that: 
    \[\lim_{n \to \infty} \prob\left(\theta \in \left[L, U\right]\right) = 1 - \alpha\]

    \subparag{Remark}{
        We can always use the CLT to make an approximate confidence interval when we are considering a sum of random variable.
    }
}

\parag{Theorem}{
    Let $Y_1, \ldots, Y_n \iid \mathcal{N}\left(\mu, \sigma^2\right)$. Then, we have the following independent estimator: 
    \[\bar{Y} \followsdistr \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)\]
    \[\sum_{j=1}^{n} \left(Y_j - \bar{Y}\right)^2 \followsdistr \sigma^2 \chi^2_{n-1}\]
    where $\chi_{\nu}^2$ represents the chi-square distribution with $\nu$ degrees of freedom (it will be defined shortly after).

    \subparag{Remark}{
        The first result implies the CLT, giving the following pivot: 
        \[Z = \frac{\bar{Y} - \mu}{\sqrt{\frac{\sigma^2}{n}}} \followsdistr \mathcal{N}\left(0, 1\right)\]

        This pivot yields the following $\left(1 - \alpha_L - \alpha_U\right)$ CI for $\mu$: 
        \[\left(L, U\right) = \left(\bar{Y} - \frac{\sigma}{\sqrt{n}} z_{1 - \alpha_L}, \bar{Y} - \frac{\sigma}{\sqrt{n}} z_{\alpha_U}\right)\]
    }
}

\parag{Interpretation}{
    It is important to understand that $\theta$ is not sampled randomly in $\left(L, U\right)$, it is the inverse. $\theta$ is fixed but we don't have access to it. However, we get some random observations, which yield the random interval $\left(L, U\right)$. 

    In other words, if we run the same experiment multiple times (with the same unknown $\theta$), we will get different observations and thus different random intervals.
}

\parag{Definition: Standard error}{
    Let $T = t\left(Y_1, \ldots, Y_n\right)$ be an estimator of $\theta$, let $\tau_n^2 = \Var\left(T\right)$ be its variance, and let $V = v\left(Y_1, \ldots, Y_n\right)$ be an estimator of $\tau_n^2$. Then we call $\sqrt{V}$, or its realisation $\sqrt{v}$, a \important{standard error} for $T$.
}

\parag{Observation}{
    In most cases, we have: 
    \[U - L \propto \sqrt{V} \propto \frac{1}{\sqrt{n}}\]

    In other words, multiplying the number of samples by 100 increases the precision by a factor 10.
}

\parag{Theorem: Empirical CLT}{
    Let $T$ be an estimator of $\theta$ based on a sample of size $n$ such that:
    \[\frac{T - \theta}{\tau_n} \over{\to}{D} Z \followsdistr \mathcal{N}\left(0, 1\right), \mathspace \frac{V}{\tau^2_n} \over{\to}{P} 1\]

    Then, we have: 
    \[\frac{T - \theta}{\sqrt{V}}\over{\to}{D} Z\]

    \subparag{Intuition}{
        This means that, if we have an approximation of the variance $V$, we can use it for the central limit theorem.

        This gives an empirical version of the CLT.
    }
    
    \subparag{Proof}{
        This proof comes directly from the hypothesis: 
        \[\frac{T - \theta}{\sqrt{V}} = \frac{T - \theta}{\tau_n} \cdot \underbrace{\frac{\tau_n}{\sqrt{V}}}_{\to 1} \over{\to}{D} Z\]

        \qed
    }
}

\parag{Example}{
    We throw a coin 200 times and observe 115 heads. We want to know if the coin is fair.

    In other words, we have $X_1, \ldots, X_n \followsdistr \text{Ber}\left(\theta\right)$ and: 
    \[S = X_1 + \ldots + X_n \followsdistr B\left(n, \theta\right)\]
    
    Using the central limit theorem, we know that: 
    \[Q = \frac{S - n \theta}{\sqrt{n \theta\left(1 - \theta\right)}} \over{\to}{D} \mathcal{N}\left(0, 1\right)\]
    
    Using our regular method, we would need to solve the following inequation for $\theta$: 
    \[-z_{1 - \frac{\alpha}{2}} \leq \frac{S - n \theta}{\sqrt{n \theta\left(1- \theta\right)}} \leq z_{1 - \frac{\alpha}{2}}\]
    
    However, this is very hard to do. We thus use another method, our theorem. Using the law of large numbers, we get that: 
    \[\frac{S}{n} \over{\to}{P} \theta\]
    
    We moreover know that: 
    \[\tau_n^2 = n \theta\left(1 - \theta\right)\]

    Now, we don't want to get rid of the $\theta$ to simplify the inequality. Thus, we can make the following guess: 
    \[V = n \frac{S}{n}\left(1 - \frac{S}{n}\right)\]
    
    This is indeed such that:
    \[\frac{V}{\tau^2_n} = \frac{\frac{S}{n}\left(1 - \frac{S}{n}\right)}{\theta\left(1 - \theta\right)} \over{\to}{P} 1\]
    
    We can thus use the empirical version of the CLT, giving a new, easier to use, approximate pivot: 
    \[Q' = \frac{S - n \theta}{\sqrt{V}} = \frac{S - n \theta}{\sqrt{n \frac{S}{n}\left(1 - \frac{S}{n}\right)}}\over{\to}{D} \mathcal{N}\left(0, 1\right)\]

    We can then use it to make our confidence integral, yielding: 
    \autoeq{-z_{1 - \frac{\alpha}{2}} \leq \frac{S - n \theta}{\sqrt{S\left(1 - \frac{S}{n}\right)}} \leq z_{1 - \frac{\alpha}{2}} \iff \frac{S}{n} - \frac{z_{1 - \frac{\alpha}{2}}}{n}\sqrt{S \left(1 - \frac{S}{n}\right)} \leq \theta \leq \frac{S}{n} + \frac{z_{1 - \frac{\alpha}{2}}}{n} \sqrt{S\left(1 - \frac{S}{n}\right)}}

    This gives that:
    \[\alpha = 0.05 \implies 0.506 \leq \theta \leq 0.644, \mathspace \alpha = 0.01 \implies 0.485 \leq \theta \leq 0.665\]
    
    In the first case, we reject that the coin is fair, whereas in the second case we don't reject. This yields the following section.
}

\subsection{Hypothesis tests}

\parag{Observation}{
    We notice that we can use confidence intervals to assess the plausibility of a value $\theta^{0}$ of $\theta$. 

    If $\theta^{0}$ lies inside a $\left(1- \alpha\right)$ confidence interval, then we cannot reject the hypothesis that $\theta = \theta^0$ at significance level $\alpha$. 

    However, if $\theta^0$ lies outside as $\left(1 - \alpha\right)$ confidence interval, then we reject the hypothesis that $\theta = \theta^0$ at significance level $\alpha$.

    \subparag{Remark}{
        The greater the $\alpha$ that allows us to reject, the surer we are that the hypothesis should be rejected if it lies outside the confidence interval, but also the more values cannot be rejected.
    }
}

\parag{Definition: Hypotheses}{
    The \important{null hypothesis}, written $H_0$, is the theory we want to test. The \important{alternative hypothesis}, written $H_1$, is the opposite of $H_0$ (what is true when $H_0$ is false).
}

\parag{Definition: Types of errors}{
    We call a \important{false positive} (or type 1 error) if we reject $H_0$ when it is in fact true. We call a \important{false negative} (or type 2 error) if we reject $H_1$ when it is in fact true.

    \subparag{Remark}{
        We are negative when we accept the null hypothesis.
    }
}

\parag{Definition: Simple and composite hypothesis}{
    A \important{simple hypothesis} entirely fixes the distribution of the data $Y$. A \important{composite hypothesis} does not fix the distribution, it leaves some degree of freedom.

    \subparag{Example}{
        For instance, let $\theta_0$ be fixed. The hypothesis ``the coin is fair'', $H_0$, is simple, but the hypothesis ``the coin is not fair'', $H_1$, is composite: 
        \[H_0: R \followsdistr B\left(n, \theta_0\right)\] 
        \[H_1 : R \followsdistr B\left(n, \theta\right), \mathspace \theta \in \left]0, 1\right[ \setminus \theta_0\]
    }
}

\parag{Definition: Size and Power}{
    Let $H_0, H_1$ be simple hypotheses.

    The false positive probability is named the \important{size} and is written $\alpha$. The true positive probability is named the \important{power}, written $\beta$: 
    \[\alpha = \prob_0\left(\text{reject } H_0\right), \mathspace \beta = \prob_1\left(\text{reject } H_0\right)\]
    
    \subparag{Remark}{
        We need $H_0$ and $H_1$ to be simple hypotheses because we want to be able to speak of $\prob_0$ and $\prob_1$, which are dependent of a parameter if $H_0$ or $H_1$ is composite.
    }
}

\parag{Example}{
    Let $T$ be a random variable. We consider the following simple null hypothesis and composite alternative hypothesis: 
    \[H_0: T \followsdistr \mathcal{N}\left(0,1\right), \mathspace H_1: T \followsdistr \mathcal{N}\left(\mu, 1\right)\]
    
    The idea is to reject $H_0$ if $T > t$, where $t$ is some cut-off. To find this threshold, we compute the probability to have a false positive (reject $H_0$ incorrectly) and true positive (reject $H_0$ correctly): 
    \[\alpha\left(t\right) = \prob_0\left(T > t\right) = 1 - \Phi\left(t\right) = \Phi\left(-t\right)\] 
    \[\beta\left(t\right) = \prob_1\left(T > t\right) = \prob\left(T - \mu > t - \mu\right) = 1 - \Phi\left(t - \mu\right) = \Phi\left(\mu - t\right)\]
    
    Now, the amount of false positive and true positive (linked to the number of false negative) we accept to have depends on why we are doing this statistical test. For instance, we would not want to diagnose someone with a sickness they don't have. However, when we are looking for sicknesses in the population to know which should be watched, we don't want to miss any.
}

\parag{Definition: ROC curve}{
    The \important{receiver operating characteristic curve} (ROC curve) of a test is the plot of $\beta\left(t\right) = \prob_1\left(T > t\right)$ against $\alpha\left(t\right) = \prob_0\left(T > t\right)$ as the cut-off $t$ varies.

    This allows to know what true positive probability we have for any given false positive probability.

    \subparag{Example}{
        For instance, in our previous example, we can draw the 4 following ROC curves for $\mu \in \left\{0, 0.4, 3, 6\right\}$:
        \imagehere[0.6]{ROCExample.png}
    }
}

\parag{Definition: Pearson statistic}{
    Let $O_1, \ldots, O_k$ be the number of observations of a random sample of size $n = n_1+ \ldots + n_k$ falling into the categories $1, \ldots, k$ with expected numbers $E_1, \ldots, E_k$.

    The \important{Pearson statistic} (also named \important{chi-square statistic}) is: 
    \[T = \sum_{i=1}^{k} \frac{\left(O_i - E_i\right)^2}{E_i}\]
}

\parag{Definition: Chi-square distribution}{
    Let $W$ be a random variable and $\nu \in \mathbb{N}^*$. $W$ follows the \important{chi-square distribution} with $\nu$ degrees of freedom, written $W \followsdistr \chi^2_{\nu}$, if it has a density: 
    \[f_W\left(w\right) = \frac{1}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} w^{\frac{\nu}{2} - 1} e^{-\frac{w}{2}}, \mathspace w > 0\]
    where $\Gamma\left(a\right)$ is the gamma function: 
    \[\Gamma\left(a\right) = \int_{0}^{\infty} u^{a-1}e^{-u}du, \mathspace a > 0\]
    
    \subparag{Intuition}{
        The PDF of this distribution is not really important, the intuition of where it comes from is much more important.

        Let $Z_1, \ldots, Z_{\nu} \iid \mathcal{N}\left(0, 1\right)$. Then, the following random variable follows a chi-square distribution with $\nu$ degrees of freedom: 
        \[W = Z_1^2 + \ldots + Z_{\nu}^2\]
    }

    \subparag{Expectation}{
        We can for instance compute the expectation very easily: 
        \[\exval\left(W\right) = \exval\left(Z_1^2\right) + \ldots + \exval\left(Z_{\nu}^2\right) = \nu\]
    }
}


\parag{Theorem}{
    Let $O_1, \ldots, O_k$ be multinomial with denominator $n$ and probabilities $\left(p_1, \ldots, p_k\right)$, where: 
    \[p_1 = \frac{E_1}{n}, \mathspace \ldots, \mathspace p_k = \frac{E_k}{n}\]
    
    Then: 
    \[T \followsdistr \chi_{k-1}^2 \]
    where $T$ is the Pearson statistic.
    
    \subparag{Remark 1}{
        The approximation is typically good if: 
        \[\frac{1}{k}\sum_{i=1}^{k} E_i \geq 5\]
    }

    \subparag{Remark 2}{
        This allows to verify if $O_1, \ldots, O_k$ indeed follow a multinomial distribution.
    }
    
    \subparag{Justification}{
        Under the multinomial distribution, we have: 
        \[\exval\left(O_i\right) = n p_i = E_i\]
        \[\Var\left(O_i\right) = np_i\left(1 - p_i\right) = E_i \left(1 - \frac{E_i}{n}\right) \approx E_i\]
        
        This means that, applying the central limit theorem, we (very approximately) have: 
        \[\frac{O_i - E_i}{\sqrt{E_i}} \approx Z_i \followsdistr \mathcal{N}\left(0, 1\right)\]
        
        Thus, Pearson's statistics is given by: 
        \[T = \sum_{i=1}^{k} \frac{\left(O_i - E_i\right)^2}{E_i} \approx \sum_{i=1}^{k} Z_i^2\]
        
        However, we only have $k-1$ degrees of freedom, since we have the following constraint coming from the multinomial distribution: 
        \[O_1 + \ldots + O_k = n\]
        
        This means that $O_1, \ldots, O_{k-1}$ are free, but $O_k$ is completely determined by the others. Thus, we are summing $k-1$ squared independent standard Gaussians, giving us: 
        \[T \followsdistr \chi_{k-1}^2 \]

        To make a formal proof, we would require the multidimensional central limit theorem.
    }
}

\end{document}
