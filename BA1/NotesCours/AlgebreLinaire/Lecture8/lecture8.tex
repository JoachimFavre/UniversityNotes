% !TeX program = lualatex

\documentclass[a4paper]{article}

% Expanded on 2021-10-18 at 13:13:48.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Lundi 18 octobre 2021}

\begin{document}
\maketitle

\lecture{8}{2021-10-18}{Fin des inverses et début des déterminants}{
\begin{itemize}[left=0pt]
    \item Définition des matrices élémentaires.
    \item Explication du grand théorème (12 propositions équivalentes pour dire qu'une matrice est inversible).
    \item Définition du déterminant pour une matrice $n \times n$.
    \item Démonstration de la formule pour les matrices $2\times2$ et $3\times3$, et les matrices triangulaires et diagonales.
\end{itemize}

}


\begin{parag}{Exemple}
    Si on a une matrice $A$ définie comme:
    \[A = \begin{bmatrix} 2 & 3 \\ 2 & 4 \end{bmatrix} \]

    On peut prendre la matrice augmentée:
    \[\begin{bmatrix} 2 & 3 & 1 & 0 \\ 2 & 4 & 0 & 1 \end{bmatrix} \]

    En prenant $\left(b\right) \leftarrow \left(b\right) - \left(a\right)$:
    \[\begin{bmatrix} 2 & 3 & 1 & 0 \\ 0 & 1 & -1 & 1 \end{bmatrix} \]

    Puis on peut prendre $\left(a\right) \leftarrow \left(a\right) - 3\left(b\right)$:
    \[\begin{bmatrix} 2 & 0 & 4 & -3 \\ 0 & 1 & -1 & 1 \end{bmatrix} \]

    Enfin, on prend $\left(a\right) \leftarrow \frac{1}{2} \left(a\right)$:
    \[\begin{bmatrix} 1 & 0 & 2 & \frac{-3}{2} \\ 0 & 1 & -1 & 1 \end{bmatrix} \]

    Donc on a:
    \[A^{-1} = \begin{bmatrix} 2 & \frac{-3}{2} \\ -1 & 1 \end{bmatrix} \]

    Ce qu'on peut vérifier simplement.
\end{parag}

\begin{parag}{Lien entre matrices et opération élémentaires}
    Pour échelonner une matrice, on effectue trois types d'opérations sur les lignes. Toutes peuvent s'écrire comme un produit matriciel avec une matrice identité à laquelle on a préalable appliqué l'opération.

    \begin{subparag}{Permuter deux lignes}
        Par exemple, pour permuter la ligne $\left(a\right)$ et la ligne $\left(b\right)$ dans $I_3$ :
        \[E = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} \]

        $E$ est la matrice identité dont la ligne $\left(a\right)$ et la ligne $\left(b\right)$ ont été permutées.

        On remarque que $E$ est inversible. En effet:
        \[E^{-1} = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix} = E\]

        Ce qui est logique, puisque pour faire l'inverse d'échanger deux lignes, on peut ré-échanger ces deux lignes.
    \end{subparag}

    \begin{subparag}{Multiplier une ligne par un coefficient non nul}
        Par exemple, si on veut multiplier la ligne $\left(c\right)$ par 5:
        \[E = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 5 \end{bmatrix} \]

        $E$ est la matrice identité, dont la ligne $\left(c\right)$ a été multipliée par $5$.

        On remarque que $E$ est inversible. En effet:
        \[E^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \frac{1}{5} \end{bmatrix} \]

        Pour faire l'inverse de multiplier une ligne par $5$, il faut la multiplier par $\frac{1}{5}$.
    \end{subparag}

    \begin{subparag}{Ajouter à une ligne un multiple d'une autre ligne}
        Par exemple, si on veut ajouter à la ligne $\left(c\right)$ la ligne $\left(a\right)$ multipliée par $4$:
        \[E = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 4 & 0 & 1 \end{bmatrix} \]

        $E$ est la matrice identité, où quatre fois la ligne $\left(a\right)$ a été ajoutée à la ligne $\left(c\right)$ .

        On remarque que $E$ est inversible. En effet:
        \[E^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -4 & 0 & 1 \end{bmatrix} \]

        Pour annuler l'effet de $E$, il faut soustraire quatre fois la ligne $\left(a\right)$ à la ligne $\left(c\right)$.
    \end{subparag}
\end{parag}

\begin{parag}{Exemple}
    Soit la matrice suivante:
    \[A = \begin{bmatrix} 2 & 3 \\ 2 & 4 \end{bmatrix} \]

    On veut soustraire la ligne $\left(a\right)$ de la ligne $\left(b\right)$. Donc, notre matrice $E_1$ est donnée par :
    \[E_1 = \begin{bmatrix} 1 & 0 \\ -1 & 1 \end{bmatrix} \implies E_1 A = \begin{bmatrix} 1 & 0 \\ -1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 3 \\ 2 & 4 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix} \]

    On a donc bien eu l'effet voulu. Maintenant, si on veut soustraire trois fois la ligne $\left(b\right)$ à la ligne $\left(a\right)$:
    \[E_2 = \begin{bmatrix} 1 & -3 \\ 0 & 1 \end{bmatrix} \implies E_2 E_1 A = E_2 \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix}  = \begin{bmatrix} 1 & -3 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \]

    Maintenant, on veut diviser la première ligne par 2, donc on a:
    \[E_3 = \begin{bmatrix} \frac{1}{2} & 0 \\ 0 & 1 \end{bmatrix} \implies E_3 E_2 E_1 A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I_2\]

    On a donc que
    \[E_3 E_2 E_1 = A^{-1}\]

    \begin{subparag}{Récapitulation}
        \begin{itemize}[left=0pt]
            \item  Les matrices $E$ ci-dessus sont appelées \important{matrices élémentaires}.
            \item Elles correspondent à des opérations élémentaires sur les lignes et elles sont inversibles.
            \item Quand on échelonne une matrice, on la multiplie à gauche par une suite de matrices élémentaires.
            \item En particulière, si $A \in \mathbb{R}^{n\times n}$ est inversible, on sait qu'elle a $n$ pivots. Dès lors, sa forme échelonne réduite est $I_n$. En d'autres mots, il existes une suite d'opérations élémentaires qui réduit $A$ en $I_n$. Donc:
            \[E_k \ldots E_2 E_1 A = I_n\]

            Comme $A$ est inversible, on peut multiplier par $A^{-1}$ à droite:
            \[E_k \ldots E_2 E_1 A A^{-1} = I_n A^{-1} \implies A^{-1} = E_k \ldots E_2 E_1\]

        \end{itemize}

        Donc, L'inverse de $A$ est le produit des matrices élémentaires qui réduisent $A$ sous forme échelonnée réduite. En pratique, quand on échelonne
        \[\begin{bmatrix}  &  &  &  &  &  \\  & A &  &  & I_n &  \\  &  &  &  &  &  \end{bmatrix} \]

        On multiplie à gauche par les matrices $E_1$, $E_2$, \ldots:
        \begin{multiequality}
        E_k \ldots E_2 E_1 \begin{bmatrix}  &  &  \\ A &  & I_n \\  &  &  \end{bmatrix} & =  \begin{bmatrix}  &  &  \\ E_k \ldots E_2 E_1 A &  & E_k \ldots E_2 E_1 I_n \\  &  &  \end{bmatrix}  \\
        & =  \begin{bmatrix}  &  &  \\ I_n &  & A^{-1} \\  &  &  \end{bmatrix} 
        \end{multiequality}
        

    \end{subparag}


\end{parag}

\begin{parag}{Théorème}
    Soit $A$ une matrice carrée $n \times n$. Les propriétés suivantes sont équivalentes, c'est-à-dire qu'elles sont soit toutes vraies, soit toutes fausses.
    \begin{enumerate}
        \item $A$ est inversible.
        \item $A$ est équivalente selon les lignes à la matrice unité $n \times n$, $I_n$.
        \item $A$ admet $n$ positions de pivot.
        \item L'équation $A \bvec{x} = \bvec{0}$ admet la solution triviale pour unique solution.
        \item Les colonnes de $A$ sont linéairement indépendantes.
        \item L'application $\bvec{x} \mapsto A \bvec{x}$ est injective.
        \item Pour tout vecteur $\bvec{b} \in \mathbb{R}^{n}$, l'équation $A \bvec{x} = \bvec{b}$ admet au moins une solution (on aurait aussi pu dire ``une solution unique'', ou au plus une solution).
        \item Les colonnes de $A$ engendrent $\mathbb{R}^{n}$.
        \item L'application linéaire $x \mapsto A \bvec{x}$ est surjective.
        \item\label{enum:inverseMultiplicationGauche} Il existe une matrice $C$ de taille $n \times n$ telle que $CA = I_n$.
        \item\label{enum:inverseMultiplicationDroite} Il existe une matrice $D$ de taille $n \times n$ telle que $AD = I_n$.
        \item $A^T$ est inversible.
    \end{enumerate}

    Les propriétés (\ref{enum:inverseMultiplicationGauche}) et (\ref{enum:inverseMultiplicationDroite}) permettent de voir que notre définition de matrice inversible --- que $A^{-1}$ doit être tel que $A A^{-1} = A^{-1} A = I_n$ --- est trop forte. Cependant, nous en avions besoin pour démontrer ces propriétés.
\end{parag}

\subsection{Déterminant}
\begin{parag}{Introduction}
    On a découvert qu'une matrice $2 \times 2$
    \[A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \]
    est inversible si et seulement si $ad - bc \neq 0$. On a appelé cette quantité, $ad - bc$, le déterminant de $A$, noté $\det A$.

    On peut faire pareil pour une matrice $1 \times 1$, $A = \begin{bmatrix} a \end{bmatrix} $. Une telle matrice est inversible si et seulement si $a \neq 0$. On définit donc $\det A = a$.

    On se demande maintenant s'il est possible de définir une quantité ``$\det A$'' pour $A$ de taille $n \times n$, avec $n$ quelconque, telle que $A$ est inversible si et seulement si $\det A \neq 0$.
\end{parag}

\begin{parag}{Définition pour une matrice $n \times n$}
    Soit $A \in \mathbb{R}^{n \times n}$:
    \[A = \begin{bmatrix} a_{11} & a_{12} & \ldots & a_{1n} \\ a_{21} & a_{22} & \ldots & a_{2n} \\ \vdots & \vdots &  & \vdots \\ a_{n1} & a_{n2} & \ldots & a_{nn} \end{bmatrix} \]

    Par $A_{ij}$, on désigne la matrice de taille $\left(n-1\right)\times \left(n -1\right)$ obtenu en effaçant la $i$-ème ligne et la $j$-ème colonne de $A$.

    Le \important{déterminant de $A$} est définit récursivement comme suit:
    \begin{itemize}
        \item Si $n = 1$, $\det A = a_{11}$.
        \item Si $n > 1$,
            \[\det A = \sum_{j=1}^{n} a_{ij} \underbrace{\left(-1\right)^{i + j} \det A_{ij}}_{\text{cofacteur $\left(i, j\right)$ de $A$}} \]
            où $i$ est une ligne qu'on a choisie. Le résultat ne dépend pas de la ligne qu'on choisit à chaque itération.
    \end{itemize}

    On peut se représenter le signe de la manière suivante, pour une matrice $5\times5$:
    \[\begin{bmatrix} + & - & + & - & + \\ - & + & - & + & - \\ + & - & + & - & + \\ - & + & {\color{red}-} & + & - \\ + & - & + & - & + \end{bmatrix} \]

    Donc, si on a $i = 4$ et $j = 3$, alors on a que $\left(-1\right)^{4 + 3} = {\color{red}-1}$.
\end{parag}

\begin{parag}{Exemple pour une matrice $2 \times 2$}
    Si on choisit $i = 1$, on a, pour $j = 1$:
    \[A_{11} = \begin{bmatrix} a_{22} \end{bmatrix} \implies \det A_{11} = a_{22}\]

    Ensuite, pour $j = 2$:
    \[A_{12} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \implies \det A_{12} = a_{21}\]

    Donc, on a que:
    \[\det A = a_{11} \left(-1\right)^{1 + 1} \det A_{11} + a_{12} \left(-1\right)^{1 + 2}\det A_{12} = a_{11} a_{22} - a_{12} a_{21} = ad - bc\]

    Si on fait le même raisonnement, mais en choisissant $i = 2$, alors on obtiendra exactement la même chose.
\end{parag}

\begin{parag}{Exemple pour une matrice $3 \times 3$}
    Soit une matrice $3 \times 3$ quelconque (ok je ferai pas la blague cette fois, mais est-ce que je l'ai faite en disant que je n'allais pas la faire ?):
    \[A = \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix} \]


    Choisissons $i = 1$. On a pour $j = 1$:
    \[A_{11} = \begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix} \implies \det A_{11} = a_{22} a_{33} - a_{23} a_{32}\]

    Pour $j = 2$:
    \[A_{12} = \begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix} \implies \det A_{12} = a_{21} a_{33} - a_{23} a_{31}\]

    Pour $j = 3$:
    \[A_{13} = \begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \implies \det A_{13} = a_{21} a_{32} - a_{22} a_{31}\]

    Ainsi, on peut obtenir le déterminant:
    \begin{multiequality}
    \det A & = a_{11} \left(-1\right)^{1+1} \det A_{11} + a_{12} \left(-1\right)^{1 + 2} \det A_{12} + a_{13} \left(-1\right)^{1 + 3} \det A_{13} \\
    & = a_{11} \left(a_{22} a_{33} - a_{23} a_{32}\right) - a_{12} \left(a_{21} a_{33} - a_{23} a_{31}\right) + a_{13} \left(a_{21} a_{32}- a_{22} a_{31}\right) \\
    & = a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33} - a_{13} a_{22} a_{31}
    \end{multiequality}
    
    On peut utiliser le moyen mnémotechnique suivant (valable uniquement pour les matrices $3 \times 3$):

    \imagehere{MoyenMneomtechniqueDeterminant3x3.png}
\end{parag}

\begin{parag}{Théorème}
    Pour $A \in\mathbb{R}^{n \times n}$ quelconque, on a
    \[\det A = \det A^{T}\]

    \begin{subparag}{Conséquence}
        Dans la définition du déterminant, on peut choisir de calculer ``l'expansion en cofacteurs'' du déterminant le long d'une ligne \important{ou} d'une colonne au choix.
    \end{subparag}

    \begin{subparag}{Exemple}
        Si on a la matrice
        \[A = \begin{bmatrix} 0 & 1 & 3 & 0 \\ 2 & 0 & 1 & 2 \\ 0 & 1 & 4 & 2 \\ 0 & 1 & 1 & 1 \end{bmatrix} \iff A^T = \begin{bmatrix} 0 & 2 & 0 & 0 \\ 1 & 0 & 1 & 1 \\ 3 & 1 & 4 & 1 \\ 0 & 2 & 2 & 1 \end{bmatrix}\]

        Utiliser la première ligne de $A^T$, c'est à dire la première colonne de $A$, est beaucoup plus simple car il y a beaucoup de zéros.
    \end{subparag}

\end{parag}

\begin{parag}{Le cas d'une matrice triangulaire}
    Si on a $A$, une matrice triangulaire, par exemple:
    \[A = \begin{bmatrix} 5 & 2 & 3 & 2 \\ 0 & -2 & 1 & 1 \\ 0 & 0 & 3 & 7 \\ 0 & 0 & 0 & 1 \end{bmatrix} \]

    Dans ce cas, si on fait l'expansion selon la première colonne, on a que
    \[\det A = 5 \det \begin{bmatrix} -2 & 1 & 1 \\ 0 & 3 & 7 \\ 0 & 0 & 1 \end{bmatrix} + 0 \ldots + 0\ldots + 0\ldots\]
    qui est à nouveau une matrice triangulaire. En répétant cette idée plusieurs fois, on se trouve avec le fait que
    \[\det A = 5 \cdot \left(-2\right) \cdot 3 \cdot 1\]
    soit le produit des éléments sur la diagonale de $A$.

    En d'autres mots, si on a une matrice triangulaire supérieure, triangulaire inférieure ou diagonale, alors son déterminant est donné par le produit des coefficients diagonaux.
\end{parag}



\end{document}
