% !TeX program = lualatex

\documentclass{article}

% Expanded on 2021-10-07 at 08:11:08.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Jeudi 07 octobre 2021}

\begin{document}
\maketitle

\lecture{5}{2021-10-07}{Début des applications linéaires}{
\begin{itemize}[left=0pt]
    \item Définition des applications et de la terminologie autour.
    \item Définition des applications linéaires et démonstration que $T$ est linéaire si et seulement s'il est sous la forme $T\left(\bvec{x}\right) = A \bvec{x}$, où $A$ est une matrice.
    \item Explication de comment calculer la transformation linéaire d'un ensemble de points.
\end{itemize}
}


\section{Introduction aux applications linéaires}
\subsection{Applications linéaires}

\begin{parag}{Introduction}
    Étant donnés un vecteur $\bvec{x} \in \mathbb{R}^{n}$ et une matrice $A \in \mathbb{R}^{m\times n}$, on a défini le produit $A \bvec{x}$ comme suit:
    \[A \bvec{x} = x_1 \bvec{a}_1 + \ldots + x_n \bvec{a}_n\]
    où $\bvec{a}_1, \ldots, \bvec{a}_n \in \mathbb{R}^m$ sont les colonnes de $A$. Donc, \important{une matrice $A \in \mathbb{R}^{m \times n}$ permet de transformer chaque vecteur de $R^{n}$ en vecteur de $R^{m}$}
\end{parag}


\begin{parag}{Définition application}
    On appelle \important{application} (ou fonction, ou transformation) de $\mathbb{R}^{n}$ vers $\mathbb{R}^{m}$ une règle $T$ qui associe à tout vecteur de $\mathbb{R}^{n}$ un vecteur de $\mathbb{R}^{m}$. On écrit:
    \[\begin{split}
    T: \mathbb{R}^{n} &\longmapsto \mathbb{R}^{m} \\
    \bvec{x} &\longmapsto T\left(\bvec{x}\right)
    \end{split}\]

    On appelle $\mathbb{R}^{n}$ l'espace de départ et $\mathbb{R}^{m}$ l'espace d'arrivée.

    \imagehere{SchemaEnsembleDepartArrivee.png}
\end{parag}

\begin{parag}{Terminologie}
    \begin{itemize}[left=0pt]
        \item $T\left(\bvec{x}\right)$ est l'image de $\bvec{x}$ par $T$.
        \item L'image de $T$ est l'ensemble des images de tous les $\bvec{x}$ par $T$.
        \item Étant donné une matrice $A \in \mathbb{R}^{m\times n}$, la règle $\bvec{x} \mapsto A \bvec{x}$ est un exemple d'application de $\mathbb{R}^{n}$ vers $\mathbb{R}^{m}$

         On appelle cela une \important{transformation matricielle}.
    \end{itemize}

\end{parag}

\begin{parag}{Exemple}
    Si on prend
    \[A = \begin{bmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{bmatrix} \]

    Alors, on a
    \[\begin{split}
    T: \mathbb{R}^{2} &\longmapsto \mathbb{R}^{3} \\
    \bvec{x} &\longmapsto T\left(\bvec{x}\right)
    \end{split}\]

    Où:
    \[T\left(\bvec{x}\right) = A \bvec{x} = \begin{bmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 - 3x_2 \\ 3x_1 + 5x_2 \\ -x_1 + 7x_2 \end{bmatrix} \]

    Donc, par exemple,
    \[\bvec{u} = \begin{bmatrix} 2 \\ -1 \end{bmatrix} \implies T\left(\bvec{u}\right) = T\left(\begin{bmatrix} 2 \\ -1 \end{bmatrix}\right) = \begin{bmatrix} 5 \\ 1 \\ -9 \end{bmatrix} \]


\end{parag}

\begin{parag}{Opération inverse}
    Étant donné $\bvec{b} \in \mathbb{R}^{m}$, existe-t-il $\bvec{x}$ dans l'espace de départ de $T$ tel que $T\left(\bvec{x}\right) = \bvec{b}$ ?

    Autrement dit \important{le vecteur $\bvec{b}$ est-il dans l'image de $T$ ?} Si un tel $\bvec{x}$ existe, est-il unique ?

    On peut répondre à cette question, en utilisant un système linéaire (naturellement, on est en algèbre linéaire) ! \smiley
\end{parag}

\begin{parag}{Exemple}
    Prenons
    \[A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} \text{ et } T\left(\bvec{x}\right) = A \bvec{x} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}  = \begin{bmatrix} x_1 \\ x_2 \\ 0 \end{bmatrix} \]

    Géométriquement, $T$ \important{projète} les points de $\mathbb{R}^{3}$ sur le plan défini par $x_3 = 0$.

    \imagehere{ProjectionVolumeEnPlan.png}

    Donc, étant donné un vecteur $\bvec{x}$, on calcule $\bvec{y} = T\left(\bvec{x}\right)$. On sait que $T\left(\bvec{y}\right)$ ne va rien faire (c'est pour ça qu'on l'appelle une projection).
\end{parag}

\begin{parag}{Application linéaire}
    On sait que les matrices ont comme propriété que
    \[A\left(\bvec{u} + \bvec{v}\right) = A \bvec{u} + A \bvec{v} \text{ et } A\left(c \bvec{u}\right) = c A \bvec{u}\]
    pour $A \in \mathbb{R}^{m \times n}$ et pour tous $\bvec{u}, \bvec{v} \in \mathbb{R}^{n}$, et pour tout $c \in \mathbb{R}$. Si on a $T\left(\bvec{x}\right) = A \bvec{x}$, alors cette application satisfait aussi ce propriétés.

    \begin{subparag}{Définition}
        Une \important{application $T$ est linéaire} si elle suit les deux propriétés suivantes:
        \begin{enumerate}
            \item $T\left(\bvec{u} + \bvec{v}\right) = T\left(\bvec{u}\right) + T\left(\bvec{v}\right)$
            \item $T\left(c \bvec{u}\right) = cT\left(\bvec{u}\right)$
        \end{enumerate}
        pour tous $\bvec{u}, \bvec{v}$ dans l'espace de départ de $T$ et pour tout $c \in\mathbb{R}$.

        On remarque que si $T\left(\bvec{x}\right) = A \bvec{x}$ avec $A$ étant une matrice, alors $T\left(\bvec{x}\right)$ est une application linéaire. On verra que, en fait, toutes les applications linéaires peuvent s'écrire sous cette forme.
    \end{subparag}
\end{parag}

\begin{parag}{Propriétés de la transformation linéaire}
    La définition a ces trois conséquences directes:
    \begin{enumerate}
        \item $T\left(\bvec{0}\right) = \bvec{0}$
        \item $T\left(c \bvec{u} + d \bvec{v}\right) = cT\left(\bvec{u}\right) + dT\left(\bvec{v}\right)$
        \item $T\left(c_1 \bvec{v}_1 + \ldots + c_p \bvec{v}_p\right) = c_1 T\left(\bvec{v}_1\right) + \ldots + c_p T\left(\bvec{v}_p\right)$
    \end{enumerate}

    En d'autres mots, la transformation linéaire d'une combinaison linéaire de vecteurs est égale à la combinaison linéaire des transformations linéaires, avec les même coefficients.

    On peut les justifier comme suit:
    \begin{enumerate}
        \item $T\left(\bvec{0} + \bvec{0}\right) = T\left(\bvec{0}\right) + T\left(\bvec{0}\right) \iff T\left(\bvec{0}\right) = T\left(\bvec{0}\right) + T\left(\bvec{0}\right) \iff T\left(\bvec{0}\right) = \bvec{0}$
        \item $T\left(c \bvec{u} + d \bvec{v}\right) = T\left(c \bvec{u}\right) + T\left(d \bvec{v}\right) = cT\left(\bvec{u}\right) + dT\left(\bvec{v}\right)$
        \item $T\left(c_1 \bvec{v}_1 + \ldots + c_p \bvec{v}_p\right) = T\left(c_1 \bvec{v}_1\right) + T\left(c_2 \bvec{v}_2 + \ldots + c_p \bvec{v}_p\right) = \ldots = c_1 T\left(\bvec{v}_1\right) + \ldots + c_p T\left(\bvec{v}_p\right)$
    \end{enumerate}
\end{parag}

\begin{parag}{Proposition}
    On sait que si $T$ est linéaire, alors $T\left(c \bvec{u} + d \bvec{v}\right) = c T\left(\bvec{u}\right) + d T\left(\bvec{v}\right)$ tient pour tous $\bvec{u}, \bvec{v}, c, d$. Cependant, la réciproque tient aussi (on peut utiliser cette propriété pour démontrer qu'une application est linéaire).

    \begin{subparag}{Démonstration}
        Laissée au lecteur.
    \end{subparag}

\end{parag}

\begin{parag}{Exemple}
    Soit $r \in \mathbb{R}$ et soit l'application suivante:
    \[\begin{split}
    T: \mathbb{R}^2 &\longmapsto \mathbb{R}^2 \\
    \bvec{x} &\longmapsto r \bvec{x}
    \end{split}\]

    On se demande si $T$ est une application linéaire. Pour la première propriété on a:
    \[T\left(\bvec{u} + \bvec{v}\right) = r\left(\bvec{u} + \bvec{v}\right) = r \bvec{u} + r \bvec{v} = T\left(\bvec{u}\right) + T\left(\bvec{v}\right)\]

    De la même manière, pour la deuxième propriété:
    \[T\left(c \bvec{u}\right) = r c \bvec{u} = c r \bvec{u} = cT\left(\bvec{u}\right)\]
    Puisque les deux propriétés tiennent, on sait que $T$ est une application linéaire.

    De plus, on appelle $T$ une \important{homothétie du plan}. Si $r > 1$, on appelle $T$ une \important{dilatation}. Sinon, si $0 \leq r \leq 1$, alors on appelle $T$ est une \important{contraction}. $r = 0$ et $r = 1$ sont des cas spéciaux.

    \imagehere{HomothetieExemple.png}
\end{parag}

\begin{parag}{Exemple}
    Considérons l'exemple
    \[T\left(\bvec{x}\right) = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} -x_2 \\ x_1 \end{bmatrix}\]

    $T$ est clairement linéaire car il est sous la forme $A \bvec{x}$. Nous avons par exemple
    \[T\left(\begin{bmatrix} 2 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 0 \\ 2 \end{bmatrix}, \mathspace T\left(\begin{bmatrix} 3 \\ 2 \end{bmatrix}\right) = \begin{bmatrix} -2 \\ 3 \end{bmatrix} \mathspace \text{et} \mathspace T\left(\begin{bmatrix} -1/2 \\ 2 \end{bmatrix}\right) = \begin{bmatrix} -2 \\ -1/2 \end{bmatrix} \]

    Géométriquement, cette matrice applique une rotation de 90° sur les vecteurs.
\end{parag}

\begin{parag}{Application sur un ensemble de points}
    Disons que nous voulons calculer l'image de tous les points dans un parallélogramme à travers une application linéaire $T$.

    Prenons par exemple $T: \mathbb{R}^2 \mapsto \mathbb{R}^2$ et un carré. On peut simplement calculer la transformation linéaire de deux des côtés du carré. En effet, tous les points du carré sont donnés par
    \[\bvec{w} = c_1 \bvec{u} + c_2 \bvec{v}\]

    où $0 \leq c_1 \leq 1$, $0 \leq c_2 \leq 1$, et $\bvec{u}$ et $\bvec{v}$ sont les côtés du carré. Donc, en calculant l'image de $\bvec{w}$ on a bien que:
    \[T\left(\bvec{w}\right) = c_1 T\left(\bvec{u}\right) + c_2 T\left(\bvec{v}\right)\]

    \imagehere{TransformationCarre.png}

    De manière générale, n'importe quelle transformation linéaire transforme un parallélogramme en parallélogramme.
\end{parag}

\begin{parag}{Exemple}
    Disons que nous avons
    \[T\left(\bvec{x}\right) = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 2x_1 \\ x_2 \end{bmatrix} \]

    et que nous voulons calculer l'image d'un carré de côté 1. Alors, nous aurons un rectangle deux fois plus long selon le premier axe et dont la dimension sur le premier axe n'aura pas changée.

    \imagehere{ExempleTransformationCarre.png}
\end{parag}

\begin{parag}{Matrice d''une application linéaire}
    Étant donné n'importe quelle matrice $A \in \mathbb{R}^{m \times n}$. On a vu que l'application
    \[\begin{split}
    T: \mathbb{R}^{n} &\longmapsto \mathbb{R}^{m} \\
    \bvec{x} &\longmapsto A \bvec{x}
    \end{split}\]
    est linéaire.

    L'opposé est aussi vrai. Nous devons donc vérifier que pour toute transformation linéaire $T: R^{n} \mapsto R^{m}$, il existe une matrice $A \in \mathbb{R}^{m \times n}$ telle que $T\left(\bvec{x}\right) = A \bvec{x}$.

    \begin{subparag}{Exemple}
        Définissons
        \[\bvec{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \mathspace \text{ et }\mathspace \bvec{e}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \]

        les colonnes de la matrice $I_2$:
        \[I_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \]

        On considère une transformation $T: \mathbb{R}^{2} \mapsto \mathbb{R}^{3}$ telle que
        \[T\left(\bvec{e}_1\right) = \begin{bmatrix} 3 \\ -1 \\ 0 \end{bmatrix} \mathspace \text{ et } \mathspace T\left(\bvec{e}_2\right) = \begin{bmatrix} 2 \\ 2 \\ 6 \end{bmatrix} \]

        Si on sait aussi que $T$ est linéaire, alors on sait tout de $T$. En effet. pour tout $\bvec{x} \in \mathbb{R}^2$, on peut écrire:
        \[\bvec{x} = \begin{bmatrix} x_1 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ x_2 \end{bmatrix} = x_1 \bvec{e}_1 + x_2 \bvec{e}_2\]

        Par la linéarité de $T$, on peut en déduire que
        \[T\left(\bvec{x}\right) = T\left(x_1 \bvec{e}_1 + x_2 \bvec{e}_2\right)= x_1 T\left(\bvec{e}_1\right) + x_2 T\left(\bvec{e}_2\right)\]

        Donc,
        \[T\left(\bvec{x}\right) = x_1 \begin{bmatrix} 3 \\ -1 \\ 0 \end{bmatrix} + x_2 \begin{bmatrix} 2 \\ 2 \\ 6 \end{bmatrix} = \begin{bmatrix} 3 & 2 \\ -1 & 2 \\ 0 & 6 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = A \bvec{x}\]
    \end{subparag}

    Plus généralement, nous avons le théorème suivant.
\end{parag}

\begin{parag}{Théorème}
    Soit $T: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ une application linéaire. Il existe une unique matrice $A$ telle que
    \[T\left(\bvec{x}\right) = A \bvec{x} \mathspace \forall x \in \mathbb{R}^{n}\]

    De plus, $A$ est la matrice de taille $m\times n $ donnée par
    \[A = \begin{bmatrix}  &  &  \\  &  &  \\ T\left(\bvec{e}_1\right) & \ldots & T\left(\bvec{e}_n\right) \\  &  &  \\  &  &  \end{bmatrix} \]

    où $\bvec{e}_1, \ldots, \bvec{e}_n$ sont les colonnes de la matrice identité $I_n$ (ils changent selon la dimension donnée par le contexte, $\bvec{e}_1 = \left[1, 0\right]^T$ en deux dimensions, mais $\bvec{e}_1 = \left[1, 0, 0\right]^T$ en trois dimensions).  On appelle $A$ la \important{matrice standard} de l'application linéaire $T$.

    \begin{subparag}{Preuve}
        Tout $\bvec{x}$ peut s'écrire comme combinaison linéaire des colonnes $\bvec{e}_1, \ldots, \bvec{e}_n$ de la matrice identité de taille $n$:
        \[\bvec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = x_1 \begin{bmatrix} 1 \\ \vdots \\ 0 \end{bmatrix} + \ldots + x_n \begin{bmatrix} 0 \\ \vdots \\ 1 \end{bmatrix} = x_1 \bvec{e}_1 + \ldots + x_n \bvec{e}_n\]

        Puisque $T$ est linéaire, on en déduit que
    \[T\left(\bvec{x}\right) = T\left(x_1 \bvec{e}_1 + \ldots + x_n \bvec{e}_n\right) = x_1 T\left(\bvec{e}_1\right) + \ldots + x_n T\left(\bvec{e}_n\right)\]

        On peut transformer cette somme en produit matrice-vecteur:
        \[T\left(\bvec{x}\right) = \begin{bmatrix} & & \\ &  &  \\ T\left(\bvec{e}_1\right) & \ldots & T\left(\bvec{e}_n\right) \\  &  & \\ & & \end{bmatrix} \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = A \bvec{x}\]

        La matrice est bien unique, puisque on a trouvé cette matrice. On a absolument aucun choix sur la manière dont on la construit, puisque $\bvec{e}_1, \ldots, \bvec{e}_n$ sont uniques, et les transformations linéaires transforment une valeur en une seule autre valeur (unique).
    \end{subparag}
\end{parag}

\begin{parag}{Exemple}
    L'homothétie $T\left(\bvec{x}\right) = 3\bvec{x}$ de $\mathbb{R}^2 \mapsto \mathbb{R}^2$ est une application linéaire. On veut trouver sa matrice standard. On a
    \[T\left(\bvec{e}_1\right) = T\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix} \right) = \begin{bmatrix} 3 \\ 0 \end{bmatrix} \mathspace \text{ et } \mathspace T\left(\bvec{e}_2\right) = \begin{bmatrix} 0 \\ 3 \end{bmatrix} \]

    Donc, en utilisant notre théorème, on a
    \[A = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix} = 3 I_2\]

    Note: on ne peut pas faire les transformations sur les lignes sur cette matrice, contrairement à ce qu'on a fait jusque là, car dans notre contexte ce n'est plus la matrice d'un système d'équation, on ne cherche plus de solution.
\end{parag}

\begin{parag}{Exemple 2}
    Soit $T: \mathbb{R}^2 \mapsto \mathbb{R}^2$ l'application qui à tout $\bvec{x} \in \mathbb{R}^2$ fait correspondre la rotation de $\bvec{x}$ autour de l'origine par un angle $\theta$, dénotée $T\left(\bvec{x}\right)$. Cette application est linéaire. En effet,
    \begin{itemize}
        \item $T\left(c \bvec{x}\right) = c T\left(\bvec{x}\right)$ est clair. Cela paraît logique que agrandir par $c$ un vecteur puis le faire tourner est la même chose que de faire tourner un vecteur puis de l'agrandir.
        \item $T\left(\bvec{u} + \bvec{v}\right) = T\left(\bvec{u}\right) + T\left(\bvec{v}\right)$ fait du sens aussi. Voir le dessin suivant:
          \imagehere{SchemaTransformationLineaireRotation.png}

    \end{itemize}

    Pour trouver cette matrice, on a seulement besoin de trouver comment le vecteur $\bvec{e}_1$ tourne et comment le vecteur $\bvec{e}_2$ tourne. En faisant un peu de géométrie, et en utilisant les identités trigonométriques, on trouve::
    \[A = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix} \]

\end{parag}



\end{document}
